---
title: "Week 3-1 linear models"
output: html_output
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(moodlequiz)
```


# Introduction to linear models

## Introduction

Linear models are one of the most powerful and widely used tools in statistics and data science. At their core, they help us understand how y varies with some predictor x.

In the previous practical, we learned about population means and how natural variation in the process of sampling creates uncertainty. Moreover, that we can estimate a confidence interval around the mean, within which we believe the ture mean to occur.

Linear models then allow us to test whether the true mean varies with another variable.

In this practical, weâ€™ll explore how to fit and interpret linear models, using real data. By the end, youâ€™ll have a solid foundation for applying these models in your own work.


## Key learning objectives

Our learning objective today are:

- **run** a **linear regression** in R using `lm()`
- **interpret** the output of a **linear regression** using:
  - `summary()`
  - `parameters()`
    - **extract** the **confidence interval** for the **slope** and **intercept** of the **regression** line
  - `estimate_means()`
- **plot** the data and the **regression** line
  - add the **confidence interval** using output of `estimate_means()`

Letâ€™s dive in! ðŸš€ 

![Image credit:  @allison_horst](pics/you-can-do-it.png){width=80%} <br>
<br>

## Setting up

**Materials:**

Everything you need for this practical is on Moodle

1.  Download the `Wk3-1-materials.zip` zip file from Moodle, from the course page
2.  Extract the zip file into your `BEES2041/` folder
3.  Unzip the file by:
    - **MacOS:** Double clicking the zip file
    - **Windows:** Right click on the zip file and click **"Extract All"**
4.  Go into the folder `Wk3-1-materials` created by extracting the zip.
5.  Click on the `Wk-3-1-linear-models.Rproj` to open the RStudio project and you're in!!!

We will be working with real-world datasets collected by researched in the School of Biological, Earth & Environmental Sciences. These are in the folder `data/`.

**Setting up: Packages:**

We will also be working with packages from the `tidyverse` and `easystats`, building on skills from previous pracs. You should have these already installed on your machines.

**Note** that when running R in the browser we need to install the packages each time we start a new session or after you've checked your answer

In case you don't have them installed, here is code for you to do so:
```{r, eval=FALSE}
# Uncomment and run only the lines below only if you have not previously installed these.
# install.packages("tidyverse")
# install.packages("easystats")
# install.packages("palmerpenguins")
```

> Remember to load the packages into R to use em! 

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(easystats)
library(palmerpenguins)
```

# LM continuous (Leaves)

## Worked example - Isaac with leaves across climates

![Meet Dr. Isaac Towers! Image credit: Data Dan](pics/isaac-plant-ecosystems.png){width=60%} <br>
From lectures, Data Dan walked us through an example of a linear model from a study conducted by Dr. Isaac Towers, a postdoc in the School of Biological, Earth & Environmental Sciences. Isaac was interested in understanding the relationship between leaf-construction (`log10_leaf_mass_per_area`) and rainfall (`log10_rainfall`). His research is published in New Phytologist, you can have a read of his original study [here](https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.19478) We will be working with the data Isaac compiled for us. 

> Specifically, Isaac wants to know: How does a leaf's mass per area change with rainfall?

Let's read in the Isaac's data. This is in your `"data/"` folder. 

```{r, message=FALSE}
data_towers <- read_csv("data/towers-2024.csv")

data_towers
```

To fit a linear model, we will be using the `lm()` function like so:

**Note**: Remember to assign the output of `lm()` into an object so R can save it for you in your environment

```{r}
towers_fit <- lm(log10_leaf_mass_per_area ~ log10_rainfall, data = data_towers)
```

## Slope or the change in y with x

> Recall from lectures the slope (aka gradient) of a linear model describes how our y variable (`log10_leaf_mass_per_area`) changes with x (`log10_rainfall`)

![Linear model components and different slope values, Image credit: Data Dan](pics/slope-explainer.png){width=80%} <br>

Let's inspect the results of your model use `summary(towers_fit)` and see how this corresponds to the image above

```{r}
summary(towers_fit)
```

**Questions:**

1. Which coefficient corresponds to the slope in the output of `summary(towers_fit)`? 

The slope is ??? and the coefficient value is ???

2. What direction is slope coefficient, what does this tell us?

The slope is ???. This indicates that as `log10_rainfall` ???, `log10_leaf_mass_per_area` ???. 

## The confidence interval

For a nicer output you can also use `parameters(towers_fit)`. This function convienently, gives us the **95% confidence intervals** of our coefficients.  

```{r}
parameters(towers_fit)
```

**Question:**

What does the 95% CI tell us here about the slope `log10_rainfall`?

`[-0.56, -0.52]` provides a range of values within which we can be 95% confident that the ??? population parameter coefficient lies. 


## Variation explained by the model (R^2 and F-statistic)

![Relationship with data, *R^2* and *F*-statistic, Image credit: Data Dan](pics/r2-and-f-value.png){width=60%}

<br>
> There are two metrics we are use to assess our model fit

The **R^2 value** describes how much variation is explained by your model:

- Measures the **proportion of variance** in the y variable (`log10_leaf_mass_per_area`) explained by the x variable(s) (`log10_rainfall`).
- A higher R^2 value indicates strong relationship between the y variable (`log10_leaf_mass_per_area`) explained by the x variable(s) (`log10_rainfall`), but *importantly* it **does not imply causation**.

The **F-statistic** compares the explained variance to the unexplained variance:

- A **high F-statistic** suggests that at least one predictor (`log10_leaf_mass_per_area`) is significantly contributing to the model.

- The **F-statistic's p-value** tells us whether our predictor variable explains a significant amount of variance in the response variable or not

<!-- TODO: @Daniel, here would be a nice spot to show off some simulated data for students to learn about the relationship between R^2 and F-statistic  -->

**Question:**

The ??? is a ??? relationship between the y variable (`log10_leaf_mass_per_area`) explained by the x variable (`log10_rainfall`). 

The ??? suggests that the predictor (`log10_leaf_mass_per_area`) explains ??? amount of variance in `log10_leaf_mass_per_area`.

## *F*-statistic and *P*-values - how likely is it to get our results by chance?

> The *p*-value is derived from the F-distribution

![Relationship between R^2, *F* and *P*, Image credit: Data Dan](pics/f-value-p-value.png){width=60%}
<br>

The *p*-value and the *F*-statistic are closely related. In the last prac, we learned to calculate **probability (p-value)** for a *t*-statistic from a standard normal distribution. We can do that same for our *F*-statistic from the *F*-distribution!

The *p*-value tells us how extreme our *F*-statistic is under the null hypothesis (which assumes the predictor has no effect). I like to think about this as:

> "In a hypothetical (and boring) world where nothing is expected to happen, what are the chances of finding our effect?"

- A smaller *p*-value means the observed *F*-statistic is **unlikely** to occur by chance, suggesting **strong evidence** against the null hypothesis. 

> In other words, in our hypothetical world, finding an effect is REALLY rare - meaning in the real world there is something interesting happening in our model.

<!-- TODO: @Daniel, here would be a nice spot to show off some simulated data for students to learn about the relationship between R^2, F-statistic AND p-values -->

**Question:**

Generally, *F*-statistic increases with the ???. A large *F*-statistic means that it's ??? in observing our effect. 

> Try not to get overwhelmed by the details, all we want you to understand is the relationship between test statistics e.g. *F*-statistic and *p*-values. You got this!

## Calculating and visualising predicted means 

Okay thats enough theory for the day... let's have go at estimating the means and confidence intervals for our y variable (`log10_leaf_mass_per_area`) for a range of values of x (`log10_rainfall`) using our linear model. We use `estimate_means(towers_fit, by = "log10_rainfall")` for this.

```{r}
means_towers <- estimate_means(towers_fit, by = "log10_rainfall")
means_towers
```

These are called model predictions and we can visualise these using:

**Notice** how we stored the output of our `ggplot` in `towers_model_predictions`, this gives us the option to add to plot incrementally

```{r}
ggplot(data_towers, aes(x = log10_rainfall, y = log10_leaf_mass_per_area)) +
  geom_point(size = 2, col = "red", alpha = 0.2, stroke = 0) +
  geom_ribbon(data = means_towers, aes(ymin = CI_low, ymax = CI_high, y = Mean), fill = "grey") +
  geom_line(data = means_towers, aes(y = Mean), col = "#EB8240", linewidth = 1) +
  theme_classic() +
  labs(
    title = "Response of woody-plants to rainfall",
    x = "log10 Rainfall",
    y = "log10 Leaf Mass per Area"
  ) 
```

VoilÃ !!! Let's talk through this code:

- `ggplot(data_towers, aes(x = log10_rainfall, y = log10_leaf_mass_per_area))` - here, we are visualising our y variable (`log10_leaf_mass_per_area`) and x variable (`log10_rainfall`)
- `geom_point(size = 2, col = "red", alpha = 0.2, stroke = 5)` - plotting the raw data using points
    - `size` controls the size of the points
    - `col` sets the colour of the points
    - `alpha` sets the transparency of the points, a low value makes them more transparent. This is good for overlapping points
    - `stroke` controls the thickness of the border of the points
- `geom_ribbon(data = means_towers, aes(ymin = CI_low, ymax = CI_high, y = Mean), fill = "grey")` - create a ribbon to represent our confidence intervals
    - `data = means_towers` we are plotting our model predictions
    - `aes(ymin = CI_low, ymax = CI_high, y = Mean)`, setting the lower and upper bounds of our CI and our mean values for our y variable (`log10_leaf_mass_per_area`) 
    - `fill = "grey"`, colour our ribbon in grey
- `geom_line(data = means_towers, aes(y = Mean), col = "#EB8240", linewidth = 1)`, plot the line for our model predictions
  - `aes(y = Mean)`, create the line using the mean values of our y variable (`log10_leaf_mass_per_area`) 
  - `col = "#EB8240"`, set a custom colour using [hexademical (a computer dialect to represent colour)](https://g.co/kgs/NJcxHVJ)
  - `linewidth = 1` controls the thickness of the line
  
Feel free to tinker with all these options to create a plot that helps us answer the question: **How does a leaf's mass per area change with rainfall?**

> Let's get you to try run a linear model, interpret the output and create a plot with a familar dataset! 


# LM categorical (Fish)

## Worked example - Jasmine, fish and energetics in a warming world

Linear regressions are incredibly flexible. Not only can we use them to make inferences about the relationship of continuous variables, we can also use them to test for **differences in means** between **discrete** or **categorical** variables using an **AN**alysis **O**f **VA**riance

> Recall the differences between  **discrete** or **categorical** variables. 

![A colourful example of  **continuous** and **discrete/categorical** variables, Image credit: @allison_horst](pics/continuous-discrete.png){width=60%} <br>
From lectures, Data Dan walked us through an example of an ANOVA from a study conducted by Dr. Jasmine  Martino, a postdoc in the School of Biological, Earth & Environmental Sciences. Jasmine was interested in understanding how temperature (`temp_treatment`) can influence fish energetics and metabolism (`smr`). She raised fish at three different temperatures treatment groups. You can read about Jasmine's research in the [Journal of Experimental Biology](https://journals.biologists.com/jeb/article/223/6/jeb217091/223685/Experimental-support-towards-a-metabolic-proxy-in). We will be working with the data Jasmine has compiled for us. 

![Introducing marine biologist - Dr. Jasmine Martino!](pics/jasmine-fish.png){width=60%} <br>

> Specifically, Jasmine wants to know: How does fish standard metabolic rate change with temperature treatments?

Let's read in the Jasmine's data. This is in your `"data/"` folder. 

## Setting up

```{r}
library(tidyverse)
library(easystats)
```

```{r, message=FALSE}
data_martino <- read_csv("data/martino-fish-smr.csv")
```

We are primarily interested how `smr` - a continuous variable changes with `temp_treatment`- a categorical variable. We can use a linear model to test for **differences in means** in `temp_treatment`. The key difference in this example is that our x variable is categorical variable.

![Using a linear model with categorial and continuous variables](pics/lm-anova.png){width=60%} <br>

Let's fit a model for `smr` using `temp_treatment` as a precitor:

```{r}
fit_martino <- lm(smr ~ temp_treatment, data = data_martino)
```

## Analysis of Variance (ANOVA)

> How do we know whether there is a treatment group effect?

To statistically test for an overall effect of `temp_treatment`, we will enlist the help of the `anova()` function. 

```{r}
anova(fit_martino)
```

Let's talk through what's happening here: 

- The `anova()` function splits up the total variation in our data into two main components: 
    1. variation explained by our model predictors `temp_treatment`. Think of this as a signal for a treatment effect
    2. unexplained variation `Residuals`. Think of this as noise in our data

## Detecting an overall effect - some maths

![Under the hood, maths behind an ANOVA](signal-to-noise.png){width=60%} <br>

For both the predictor (`temp_treatment`) and `Residuals` we can do some maths (but we use R instead) to calculate: 

- sum of squares (`Sum sq`)
- mean square (`Mean sq`) 

Once we have these values, we can calculate the `F-value` - a ratio between the `Mean sq` of the predictor (`temp_treatment`) and the `Mean sq` for the `Residuals`. 

In other words, the `F-value` is ratio of signal to noise: 

- **larger** values indicate a **stronger** signal
- **smaller** values means that we have a **noise** in our data.

From there, we use the *F*-statistic to retrieve the probability of observing our effect in a hypothetical (and boring) world where nothing is expected to happen - just like we did in earlier examples. 

From the output in the `anova()` function, we can see that: 

- The `F-value` is relatively large and *p*-value is very small. 

**Question:** 

What does this mean? 

- This indicates that variation explained by our model is ??? unexplained variation. 

- In other words, there is ??? signal for a treatment effect and it is very ??? to observe this effect.  

## Estimate means for each group

> Okay cool, we have an effect... how big is this effect on our y variable `smr`?

Similar to our earlier linear models, we use `estimate_means()` to calculate the mean `smr` for each `temp_treatment` group. 

```{r}
estimate_means(fit_martino, by = "temp_treatment")
```

## Slopes when x is a categorical variable are less useful

Let's take a look at the `parameters()` / `summary()` output. Looks different huh? 

```{r}
parameters(fit_martino)
```

Unlike previous examples, we have an extra row in the output! This is because our model is  fitting each `temp_treatment` group as x predictors.  

![Categorical variables becomes multiple predictors in a linear model](pics/lm-multiple-predictors.png){width=60%} <br>
Importantly: 

- `(Intercept)` represents the **mean** of `temp_treatment [20C]`. By default, the `lm()` function sorts the categorical variable alphabetically and by increasing numeric order and then chooses the first value as `(Intercept)` like this:

```{r}
data_martino$temp_treatment |>  unique()
```

- The slope for `temp_treatment [24C]` represents the slope or **difference** in means between `temp_treatment [24C]` and the `(Intercept)` i.e. `temp_treatment [20C]`.

- The slope for `temp_treatment [28C]` represents the slope or **difference** in means between `temp_treatment [28C]` and the `(Intercept)` i.e. `temp_treatment [20C]`.

## Estimate pairwise comparisons in mean differences (contrasts)

> What if we want to change the reference group from temp_treatment [20C] to temp_treatment [28C]? 

We can use the `estimate_constrasts()` function to give us pairwise comparisons between all groups. Try it out

```{r}
estimate_contrasts(fit_martino, contrast = "temp_treatment")
```
