---
title: "Week 3-1 linear models"
output: html_output
#   moodlequiz::moodlequiz:
#     replicates: 1
#   html_document:
#     toc: true
# moodlequiz:
#  category: "Week 3-1 linear models"
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)

# For the prac
library(tidyverse)
library(easystats)
library(palmerpengiuns)

# Data for prac
data_towers <- read.csv("week 3/Wk3-1-moodle/data/towers-2024.csv")

# webr counter
reditor_count <- 1

class_id <- function() {
  reditor_count <<- reditor_count + 1
  sprintf("r-editor-%d", reditor_count)
}
```

<!-- Examples

- present 2 +examples of linear regression
- for each, link the question being asked to statistcal method.
- run the linear regression for each examples, relate the output to question being asked
  - check can access various parts of output

-->

# Introduction to linear models

Linear models are one of the most powerful and widely used tools in statistics and data science. At their core, they help us understand relationships between variables. 

In this prac, weâ€™ll explore how to fit and interpret, using real data. By the end, you'll have a solid foundation for applying these models in your own work. 

### Key learning objectives

We've got quite to get through today, our learning objective today are:

- **run** a **linear regression** in R using `lm()`
- **interpret** the output of a **linear regression** using:
  - `summary()`
  - `parameters()`
    - **extract** the **confidence interval** for the **slope** and **intercept** of the **regression** line
  - `report()`
  - `estimate_means()`
- **plot** the data and the **regression** line
  - add the **confidence interval** using output of `estimate_means()`

Letâ€™s dive in! ðŸš€ 

![Image credit:  @allison_horst](you-can-do-it.png){width=80%}
<br>

### Setting up

**Materials:**

Everything you need for this practical is on Moodle

1.  Download the `Wk3-1-materials.zip` zip file from Moodle, from the course page
2.  Extract the zip file into your `BEES2041/` folder
3.  Unzip the file by:
    - **MacOS:** Double clicking the zip file
    - **Windows:** Right click on the zip file and click **"Extract All"**
4.  Go into the folder `Wk3-1-materials` created by extracting the zip.
5.  Click on the `Wk-3-1-linear-models.Rproj` to open the RStudio project and you're in!!!

We will be working with real-world datasets collected by researched in the School of Biological, Earth & Environmental Sciences. These are in the folder `data/`.

**Setting up: Packages:**

We will also be working with packages from the `tidyverse` and `easystats`, building on skills from previous pracs. You should have these already installed on your machines.

**Note** that when running R in the browser we need to install the packages each time we start a new session or after you've checked your answer

In case you don't have them installed, here is code for you to do so:
```{r, eval=FALSE}
# Uncomment and run only the lines below only if you have not previously installed these.
# install.packages("tidyverse")
# install.packages("easystats")
# install.packages("palmerpenguins")
```

> Remember to load the packages into R to use em! 

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(easystats)
library(palmerpengiuns)
```

# A Refresher: Population means and sampling 

In the previous prac, we learned about population means and the process of sampling. Here is a quick visualisation to illustrate the key message: 

> When we take a sample, our sample mean will always be taken with some error

```{r}
# Set the true mean and sd for the population
true_mean <- 5
true_sd <- 2

# Create a tibble with the population and sample means
# Take a sample of 1000 observations from a population with mean 5 and standard deviation 2
data1 <- tibble(
  value = rnorm(1000, mean = true_mean, sd = true_sd),
  type = "Population of 1000"
)

# take 1000 samples of 10 observations from a population with mean 5 and standard deviation 2
data2 <- tibble(
  value = replicate(1000, mean(rnorm(10, mean = true_mean, sd = true_sd))),
  type = "Sample mean"
)

# combine the population and sample means
data <- bind_rows(data1, data2)

# plot the distribution of the population and sample means
data |>
  ggplot(aes(x = value, fill = type)) +
  geom_histogram(binwidth = 0.5, color = "black", alpha = 0.5) +
  facet_wrap(~type) +
  labs(title = "Distribution of population and sample means", x = "Value", y = "Frequency")
```

This is the nature of sampling and for this reason, we estimate **confidence intervals** to capture the uncertainty in our samples. Let's move on 

# Isaac with leaves across climates

![Meet Isaac! Image credit: Data Dan](isaac-plant-ecosystems.png){60%}
From lectures, Data Dan walked us through an example of a linear model from a study conducted by Dr. Isaac Towers, a postdoc in the School of Biological, Earth & Environmental Sciences. Isaac was interested in understanding the relationship between leaf-construction (`log10_leaf_mass_per_area`) and rainfall (`log10_rainfall`). His research is published in New Phytologist, you can have a read of his original study [here](https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.19478) We will be working with the data Isaac compiled for us. 

> Specifically, Isaac wants to know: How does a leaf's mass per area change with rainfall?

Let's read in the Isaac's data. This is in your `"data/"` folder. 

```{r, eval=FALSE}
data_towers <- read.csv("data/towers-2024.csv")
```

To fit a linear model, we will be using the `lm()` function like so:

**Note**: Remember to assign the output of `lm()` into an object so R can save it for you in your environment

```{r}
towers_fit <- lm(log10_leaf_mass_per_area ~ log10_rainfall, data = data_towers)
```

### Slope or the change in y with x

> Recall from lectures the slope (aka gradient) of a linear model describes how our y variable (`log10_leaf_mass_per_area`) changes with x (`log10_rainfall`)

![Linear model components and different slope values, Image credit: Data Dan](slope-explainer.png)

Let's inspect the results of your model use `summary(towers_fit)` and see how this corresponds to the image above

```{r}
summary(towers_fit)
```

**Questions:**

1. Which coefficient corresponds to the slope in the output of `summary(towers_fit)`? 

The slope is `r cloze("log10_rainfall", c("(Intercept)", "log10_rainfall"))` and the coefficient value is `r cloze("-0.543846", c("3.805973", "-0.543846", "<2e-16"))`

2. What direction is slope coefficient, what does this tell us?

The slope is `r cloze("negative", c("negative", "positive"))`. This indications that the as `log10_rainfall` `r cloze("increases", c("decreases", "increases"))`, `log10_leaf_mass_per_area` `r cloze("decreases", c("decreases", "increases"))`. 

### The confidence interval

For a nicer output you can also use `parameters(towers_fit)`. This function convienently, gives us the **95% confidence intervals** of our coefficients.  

```{r}
parameters(towers_fit)
```

**Question:**

What does the 95% CI tell us here about the slope `log10_rainfall`?

`[-0.56, -0.52]` provides a range of values within which we can be 95% confident that the `r cloze("true", c("sample", "true"))` population parameter coefficient lies. 


### Variation explained by the model (R^2 and F-statistic)

![Relationship with data, *R^2* and *F*-statistic, Image credit: Data Dan](r2-and-f-value.png)
> There are two metrics we are use to assess our model fit

The **R^2 value** describes how much variation is explained by your model:

- Measures the **proportion of variance** in the y variable (`log10_leaf_mass_per_area`) explained by the x variable(s) (`log10_rainfall`).
- A higher R^2 value indicates strong relationship between the y variable (`log10_leaf_mass_per_area`) explained by the x variable(s) (`log10_rainfall`), but *importantly* it **does not imply causation**.

The **F-statistic** compares the explained variance to the unexplained variance:

- A **high F-statistic** suggests that at least one predictor (`log10_leaf_mass_per_area`) is significantly contributing to the model.

- The **F-statistic's p-value** tells us whether our predictor variable explains a significant amount of variance in the response variable or not

<!-- TODO: @Daniel, here would be a nice spot to show off some simulated data for students to learn about the relationship between R^2 and F-statistic  -->

We can use the `report(towers_fit)` provides us with a written summary of our model output

```{r}
report(towers_fit)
```

**Question:**

The `r cloze("R squared value", c("F-statistic", "R squared value", "P value"))` is a `r cloze("moderate", c("weak", "moderate", "strong"))` relationship between the y variable (`log10_leaf_mass_per_area`) explained by the x variable (`log10_rainfall`). 

The `r cloze("F-statistic", c("F-statistic", "R squared value", "P value"))` suggests that the predictor (`log10_leaf_mass_per_area`) explains `r cloze("significant", c("non-significant", "non-significant"))` amount of variance in `log10_leaf_mass_per_area`.

### *F*-statistic and *P*-values - how likely is it to get our results by chance?

> The *p*-value is derived from the F-distribution

![Relationship between R^2, *F* and *P*, Image credit: Data Dan](f-value-p-value.png)
The *p*-value and the *F*-statistic are closely related. In the last prac, we learned to calculate **probability (p-value)** for a *t*-statistic from a standard normal distribution. We can do that same for our *F*-statistic from the *F*-distribution!

The *p*-value tells us how extreme our *F*-statistic is under the null hypothesis (which assumes the predictor has no effect). I like to think about this as:

> "In a hypothetical (and boring) world where nothing is expected to happen, what are the chances of finding our effect?"

- A smaller *p*-value means the observed *F*-statistic is **unlikely** to occur by chance, suggesting **strong evidence** against the null hypothesis. 

> In other words, in our hypothetical world, finding an effect is REALLY rare - meaning in the real world there is something interesting happening in our model.

<!-- TODO: @Daniel, here would be a nice spot to show off some simulated data for students to learn about the relationship between R^2, F-statistic AND p-values -->

**Question:**

Generally, *F*-statistic increases with the `r cloze("p-value", c("the intercept", "the slope", "p-value"))`. A large *F*-statistic means that it's `r cloze("unlikely", c("not big deal", "unlikely"))` in observing our effect. 

> Try not to get overwhelmed by the details, all we want you to understand is the relationship between test statistics e.g. *F*-statistic and *p*-values. You got this!

### Calculating and visualising predicted means 

Okay thats enough theory for the day... let's have go at estimating the means and confidence intervals for our y variable (`log10_leaf_mass_per_area`) for a range of values of x (`log10_rainfall`) using our linear model. We use `estimate_means(towers_fit, by = "log10_rainfall")` for this.

```{r}
means_towers <- estimate_means(towers_fit, by = "log10_rainfall")
means_towers
```

These are called model predictions and we can visualise these using:

**Notice** how we stored the output of our `ggplot` in `towers_model_predictions`, this gives us the option to add to plot incrementally

```{r}
towers_model_predictions <-
  ggplot(data_towers, aes(x = log10_rainfall, y = log10_leaf_mass_per_area)) +
  geom_point(size = 2, col = "red", alpha = 0.2, stroke = 0) +
  geom_ribbon(data = means_towers, aes(ymin = CI_low, ymax = CI_high, y = Mean), fill = "grey") +
  geom_line(data = means_towers, aes(y = Mean), col = "#EB8240", linewidth = 1) +
  theme_classic() +
  labs(
    title = "Response of woody-plants to rainfall",
    x = "log10 Rainfall",
    y = "log10 Leaf Mass per Area"
  ) 

towers_model_predictions
```

VoilÃ !!! Let's talk through this code:

- `ggplot(data_towers, aes(x = log10_rainfall, y = log10_leaf_mass_per_area))` - here, we are visualising our y variable (`log10_leaf_mass_per_area`) and x variable (`log10_rainfall`)
- `geom_point(size = 2, col = "red", alpha = 0.2, stroke = 5)` - plotting the raw data using points
    - `size` controls the size of the points
    - `col` sets the colour of the points
    - `alpha` sets the transparency of the points, a low value makes them more transparent. This is good for overlapping points
    - `stroke` controls the thickness of the border of the points
- `geom_ribbon(data = means_towers, aes(ymin = CI_low, ymax = CI_high, y = Mean), fill = "grey")` - create a ribbon to represent our confidence intervals
    - `data = means_towers` we are plotting our model predictions
    - `aes(ymin = CI_low, ymax = CI_high, y = Mean)`, setting the lower and upper bounds of our CI and our mean values for our y variable (`log10_leaf_mass_per_area`) 
    - `fill = "grey"`, colour our ribbon in grey
- `geom_line(data = means_towers, aes(y = Mean), col = "#EB8240", linewidth = 1)`, plot the line for our model predictions
  - `aes(y = Mean)`, create the line using the mean values of our y variable (`log10_leaf_mass_per_area`) 
  - `col = "#EB8240"`, set a custom colour using [hexademical (a computer dialect to represent colour)](https://g.co/kgs/NJcxHVJ)
  - `linewidth = 1` controls the thickness of the line
  
Feel free to tinker with all these options to create a plot that helps us answer the question: **How does a leaf's mass per area change with rainfall?**

> Let's get you to try run a linear model, interpret the output and create a plot with a familar dataset! 

# Bringing back the Palmer penguins for allometry

![We can learn so much from these cuties. Image credit: @allison_horst](palmer_penguins.png)
Allometry in biology is the study of how biological traits (such as bill depth, or flipper length) scale with body size. Let's understand these relationships in our Palmer penguins using linear models!

> We are want try answer the question: **How does bill length scale with body mass in Palmer Penguins?**

Over to you - using the code from Isaac's study above, can you try: 

- Run a linear model for `bill_length_mm` and `body_mass_g`
- Generate the summary output for your model using `summary()`, `parameters()` and `report()`
  - Make some interpretations from this output using the: 
      - slope
      - confidence interval of the slope
      - R^2 value
      - F-statistic value
      - p-value
- Generate some model predictions from your model using `estimate_means()`
- Create a data visualisation showing off the `penguins` data, your predicted means from your linear
model
  - Plot the raw data with `geom_point()`
  - Add the confidence intervals with `geom_ribbon()`
  - Add the predicted linear relationship with `geom_line()`
  - Choose some [fun colours](https://sites.stat.columbia.edu/tzheng/files/Rcolor.pdf) if you would like!
  - Add a title
  - Add nice axes labels

```{r}
library(tidyverse)
library(easystats)
library(palmerpengiuns)
```

```{r}
penguins
```

```{r penguins-answer, include=FALSE}
# Fit the model
penguins_fit <- lm(bill_length_mm ~ body_mass_g, data = penguins)

# Summary of model
summary(penguins_fit)

# Clean, nice summary
parameters(penguins_fit) 

# Get full values for confidence intervals of coefficients
confint(penguins_fit)

# Text report of model
report(penguins_fit)

# Create model predictions
penguins_means <- estimate_means(penguins_fit, by = "body_mass_g")

# Make a scatterplot of raw data and model predictions
penguins_model_predictions <-
  ggplot(penguins, aes(x = body_mass_g, y = bill_length_mm)) +
  geom_point(size = 2, col = "cornflowerblue", alpha = 0.5, stroke = 0) +
  geom_ribbon(data = penguins_means, aes(ymin = CI_low, ymax = CI_high, y = Mean), fill = "azure2") +
  geom_line(data = penguins_means, aes(y = Mean), col = "#EB8240", linewidth = 1) +
  theme_classic() +
  labs(
    title = "Allometry of bill length with body mass in Palmer Penguins",
    x = "Body mass (g)",
    y = "Bill length (mm)"
  ) 

penguins_model_predictions
```

> Once you've done the above in your own RStudio, use your outputs to answer the relevant questions (Moodle).

**Questions: **

1. What does the slope tell us? 

There is a `r cloze("postive", c("postive","negative"))` relationship between bill length and body mass

2. Let's try interpret the slope. 

- A unit increase (gram) in body mass corresponds to a `r cloze("4.051e-03", c("2.690e+01","4.051e-03", "<2e-16"))` in bill length (mm)

3. Yikes, looks like by rounding the confidence intervals for the slope in the output of `parameters()` is not very helpful. Take a look at `report()` instead OR you can try `confint(penguins_fit)` to look at the unrounded values of the confidence intervals. What do the confidence intervals for the slope tell us? 

The confidence intervals for the slope tells us that the true population mean for slope lies between
`r cloze("0.0035 and 0.0046", c("24.40 and 29.40","0.0035 and 0.0046"))`.

4. Let's try understand the *R^2* and *F*-statistic values from your model. What are they telling us?

- The *R^2* value suggests that there is a `r cloze("moderate", c("weak","moderate", "strong"))` relationship between bill length and body mass. 

- The *F*-statistic is relatively small/large, meaning that bill length/bill depth/flipper length 

5. Looking at the *p*-value, how likely is it to observe our result? Do we have evidence of a relationship between bill length and body mass?

The *p*-value for slope `body_mass_g` is very `r cloze("small", c("small","large"))`, this means that the probability of observing our slope estimate is very `r cloze("unlikely", c("likely","unlikely"))` if there was no relationship between bill length and body mass. This indicates that we have `r cloze("strong evidence", c("no evidence", weak evidence","moderate evidence", "strong evidence"))` for a `r cloze("postive", c("postive","negative"))` relationship between bill length and body mass.

# What contributes to ground level Ozone?

![What is pollution doing to our ozone, Image credit: [EPA](https://www.climatecentral.org/climate-matters/ozone-pollution-the-good-the-bad-and-the-dirty)](ozone.png){50%}

The `airquality` dataset in R contains daily measurements of air pollution and meteorological conditions in New York (Mayâ€“September 1973).

```{r}
library(tidyverse)
library(easystats)
```

```{r}
airquality
```

- Ozone (Ozone, ppb): Measures ground-level ozone concentration in parts per billion (ppb). High levels indicate poor air quality and can impact human health.
- Temperature (Temp, Â°F): Records daily maximum temperature in degrees Fahrenheit. 

> We want to know whether higher temperatures can contribute to increased ozone levels due to photochemical reactions.

Using the `airquality` dataset:

- Run a linear model for the relevant variables for our question
- Generate the summary output for your model using `summary()`, `parameters()` and `report()`
  - Make some interpretations from this output using the: 
      - slope
      - confidence interval of the slope
      - R^2 value
      - F-statistic value
      - p-value
- Generate some model predictions from your model using `estimate_means()`
- Create a data visualisation showing off the `airquality` data, your predicted means from your linear
model
  - Plot the raw data with `geom_point()`
  - Add the confidence intervals with `geom_ribbon()`
  - Add the predicted linear relationship with `geom_line()`
  - Choose some [fun colours](https://sites.stat.columbia.edu/tzheng/files/Rcolor.pdf) if you would like!
  - Add a title
  - Add nice axes labels
  
> Once you've done the above in your own RStudio, use your outputs to answer the relevant questions (Moodle).

```{r}
airquality

ozone_fit <- lm(Ozone ~ Temp, data = airquality) 

summary(ozone_fit)
parameters(ozone_fit)
report(ozone_fit)

ozone_means <- estimate_means(ozone_fit, by = "Temp")

ozone_model_predictions <- ggplot(airquality, aes(x = Temp, y = Ozone)) +
  geom_point(size = 2, col = "darkgoldenrod1", stroke = 0) +
  geom_ribbon(data = ozone_means, aes(ymin = CI_low, ymax = CI_high, y = Mean), fill = "darkorange", alpha = 0.5) +
  geom_line(data = ozone_means, aes(y = Mean), col = "forestgreen", linewidth = 1) +
  labs(title = "Ozone vs Temperature with Linear Regression Line",
       x = "Temperature (F)",
       y = "Ozone Level (ppb)") +
  theme_minimal()

ozone_model_predictions
```

**Questions:**

1. 
2.
3. 

# Introduction to ANOVA

In this week's lectures, we learned that linear regression are incredibly flexible. Not only can we use them to make inferences about the relationship of continuous variables, we can also use them to test for **differences in means** between **discrete** or **categorical** variables. 

> Recall the differences between  **discrete** or **categorical** variables. 

![A colourful example of  **continuous** and **discrete/categorical** variables, Image credit: @allison_horst](continuous-discrete.png){70%}

```{r}
library(tidyverse)
library(easystats)
library(palmerpenguins)
```

# Jasmine, fish, temperature and metabolism

<!-- TODO: @Daniel, would you ask Jasmine nicely for her MR data please? Thanks! I may be able to extract it from [Fig 3A](https://journals.biologists.com/jeb/article/223/6/jeb217091/223685/Experimental-support-towards-a-metabolic-proxy-in
) using [{metadigitise}](https://github.com/daniel1noble/metaDigitise) time permitting
-->

# Mean differences in Palmer Penguins

```{r}
penguins|> select(x = island, y = Sepal.Width)

fit <- lm(y ~ x, data = data_iris_anova_3grp)

summary(fit)

parameters(fit)

means <- estimate_means(fit, by = "x")

anova(fit)
report(fit)

estimate_contrasts(fit, contrast = "x")

# Plot with raw data points
  ggplot(means, aes(x = x, y = Mean)) +
  geom_jitter(data = data_iris_anova_3grp, aes(x = x, y = y), color = "red", width = 0.1) + # Raw data
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high), width = 0, color = "grey", size = 3) + # Confidence intervals
  geom_point(size = 5, color = "orange") + # Estimated means
  theme_classic() + # Style plot
  labs(y = "y")
```


# Insecticides and crops

```{r}
# Load the dataset
InsectSprays

# Fit a linear model treating spray as a categorical predictor
model <- lm(count ~ spray, data = InsectSprays)

# Perform ANOVA
anova_result <- anova(model)

# Print results
print(anova_result)

# Check model summary
summary(model)
```