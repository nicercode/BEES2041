---
title: "Mixed models"
format:
  html:    
    self-contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

# Set the working directory
rprojroot::has_file("BEES2041-code.Rproj") |>
  rprojroot::find_root() |>
  file.path("week 5/Wk5-2-moodle/") |>
  setwd()

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)

# For prac
library(tidyverse)
library(easystats)
library(glmmTMB)

data_urchins <- read_csv("data/Urchins.csv")  |> 
  mutate(
    logBareRock = log(BareRock + 1),
    Quadrat = as.factor(Quadrat),
  )
```

# Introduction to Mixed Models

![Bringing all your R skills together!](images/witches.png){width=60%} <br>

In many real-world datasets, our observations are not entirely independent â€” measurements might be grouped by subjects, locations, time points, or experimental units. Ignoring this structure can lead to biased estimates and misleading results. **Mixed models** help us handle this complexity by explicitly modeling both **fixed effects** â€” the overall trends we're interested in â€” and **random effects** â€” addional variation due to these groupings.

# Fixed and Random effects

In the previous pracs, we have been working with **fixed effects**. These are the variables that we are interested in and whose effects are relevant across the entire population. For example, in the `urchins` dataset, we were interested in the effect of `Treatment` on `logBareRock`.

A **random effect** is: 

- categorical
- has a large number of levels _but_,
- only a **random subsample** of levels is included in your design and, 
- you want to **make inference in general** and not only for the levels you observed.

You will need to use mixed effect models if you have a random factor in your experimental design.


## Key learning objectives

Our learning objectives today are:

- **understand** differences between fixed and random effects
- **understand** difference between random intercept and random slope
- **run** a linear mixed effects model in R using `glmmTMB()`
- **interpret** the output of your multiple regression using `parameters()`, `estimate_expectations()`, `performance()`
- **understand** differences between marginal and conditional R^2^ values.
- **make model predictions** using your mixed model using `estimate_expectations()`
- **extract** the mean and confidence interval for the slope and intercept of the regression line
- **plot** the data and the regression line with confidence intervals

Letâ€™s dive in! ðŸš€ 

## Setting up: Materials

Everything you need for this prac is on Moodle

1. Download this week's materials zip file from Moodle, from the course page
2. Unzip the file by: 
  - MacOS: Double clicking the zipfile 
  - Windows: Right click on the zip file and click "Extract All" 
3. Move the extracted folder into the folder where you store materials for `BEES2041/` 
4. **Click on the Rstudio project file, eg. `Wk5-2-mixed-models.Rproj`** to open the RStudio project and you're in!!!

We will be working with various datasets collected. These are in the folder `data/`.

You will work in the relevant Quarto document for this prac. Within each Quarto docs there are several challenges for you to complete on your own devices in order to **answer the questions on Moodle**.

## Setting up: Packages

> We will be using two new package todays called `glmmTMB` and `remotes`. Uncomment the last two lines in the next code block to install it in your own devices.

```{r, eval=FALSE}
# install.packages("tidyverse")
# install.packages("easystats")
# install.packages("patchwork")

# install.packages("remotes")
# install.packages("glmmTMB")
```

> Remember to load the packages into R to use em!

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(easystats)
library(patchwork)
library(glmmTMB)
```

## Worked Example: Estuaries

![](images/estuaries.jpg){width=60%} <br>

Random factors are hard to get your head around, and are best explained with examples. The data we will analyse here are **counts of invertebrates at 3-4 sites pristine and modified sites in each of 7 (randomly chosen) estuaries**.

**Our research question is: How does total inverebrate counts differ between pristine and modified sites across all estuaries?**

Here the estuaries are the **random effect**, as there are **a large number of possible estuaries**, and we only sampled a few from the wider population of them, but we would like to make inference about estuaries in general.

**Load data**

Let's read in the dataset first

```{r}
data_estuaries <- read_csv("data/Estuaries.csv")

data_estuaries
```

**Summarise data**

A handy way to understand the structure of your study design is to create a table of counts. Here we are tabulating `Modification` by `Estuary` so we have a breakdown of how many samples we have in each category. 

```{r}
data_estuaries |> 
  count(Modification, Estuary)
```

> Notice that `Estuary` is not replicated across both levels of `Modification`. This is because, `Estuary` is not our primary variable of interest. We collected data we needed from randomly selected estuaries. 

Importantly, we know that our **random sample of estuaries** is likely to harbour different abundances of invertebrates. For example, some estuaries may be more productive, or climate at some estuaries may be more favourable to invertebrates than others. Ultimately, we know that different estuaries are likely to differ, on average in `Total` number of invertebrates. That will mean the points in our data will be more similar to each other if they come from the same estuary than if they come from different estuaries. This random effect of `Estuary` is therefore something we want to account for in our model. 

## Fit our mixed model

Let's fit the model together and we will talk through the code. For mixed models, we will be using the `glmmTMB` package and function. The way we write a model is very similar to what we have been doing in `lm()` and `glm()`.

```{r}
fit_estuaries <- glmmTMB(Total ~ Modification + (1 | Estuary), data = data_estuaries)
```

- `Total` is our numeric, continuous response variable (y) 
- `Modification` is our categorial predictor variable (x)
- `(1 | Estuary)` is a **random intercept**. This tells the model that we are fitting an intercpet that is allowed to vary by Estuary (mean `Total` at each `Estuary`).

> Technically `Total` count data, but it's distribution doesn't really resemble a Poisson distribution. A Normal distribution fits it quite well so we can use a normal repsonse here.

We can proceed with checking our model fit before looking at the model's output. Things are looking okay!

```{r}
# Check our model
check_model(fit_estuaries) 
```

Not perfect, but good enough! Let's move on to the next step.

## Compute estimated means

By now, you are familar with `estimate_means()` and `estimate_contrasts()`. For mixed models, we need to use a different function `estimate_expectations()` to compute means for each level of our predictors and random effect. 

```{r}
means_by_mod_estuaries <- estimate_expectation(fit_estuaries)  

head(means_by_mod_estuaries)
```

There are a lot of repeated rows here! This is because `estimate_expectations()` generates mean predictions for the same number of rows and replicates we had in the data. You can verify this by counting the number of rows for `Modification` and `Estuary` and the values should match with our earlier table of counts.

```{r}
means_by_mod_estuaries |> 
  count(Modification, Estuary)
```

> Let's use these means and CI to visualise our data!

## Visualise our data

Here we are plotting our raw data across different estuaries and facetting the plot by our variable of interest `Modification`.

```{r}
ggplot(data_estuaries, aes(x = Estuary, y = Total, colour = Modification, fill = Modification)) +
  geom_violin(alpha = 0.2) +
  geom_jitter(width = 0.3, alpha = 0.5) + 
  geom_errorbar(data = means_by_mod_estuaries, 
                aes(x = Estuary, y = Predicted, ymin = CI_low, ymax = CI_high),
                width = 0.15,
                col = "azure4") +
  geom_point(data = means_by_mod_estuaries, aes(y = Predicted), colour = "azure4", size = 2) + 
  scale_fill_manual(values = c("burlywood4", "cadetblue")) + 
  scale_color_manual(values = c("burlywood4", "cadetblue")) + 
  theme_classic() + 
  facet_grid(~Modification, scales = "free") + 
  # facet_wrap(~Modification) +
  theme(legend.position = "none")
```

There are three things that might be new to you in this code:

- `scale_color/fill_manual(values = c("burlywood4", "cadetblue"))` allows me to set the colour and fill manually for my plot
- `facet_grid(~Modification, scales = "free")` is very similar to `facet_wrap()` but the only difference it allows the x and y axes to differ in values. This is useful here because we didn't sample at the same estuaries for both `Modified` and `Pristine` sites. You can replace `facet_grid()` with `facet_wrap()` and see for yourself! 
- `theme(legend.position = "none")` hides the legend. The legend here is redundant to the facet title so I decided to ditch it. 

> So let's go ahead and test whether `Modification` has an effect on `Total` invertebrate abundance.

## Test for an effect of `Modification`

Since this is a simple model with 1 fixed effect, we can use the `parameters()` output but if you prefer, you can also use `estimate_contrasts()` to see whether we have a signficant difference between `Modified` and `Pristine` sites. The numbers are the same.

```{r}
parameters(fit_estuaries)
estimate_contrasts(fit_estuaries)
```

**Questions:**

**1. What does the Intercept represent in the parameters output?** 

The `Intercept` represents the ??? `Total` number of invertebrates from the ??? site.

**2. Intepret the `Modification [Pristine]` cofficient and p-value**

The `Modification [Pristine]` coefficient is ???. This value represents the mean difference in `Total` number of invertebrates between `Modified` and `Pristine` sites. The value is ??? which means the mean in `Total` is ??? for `Pristine` sites.

## Your turn: Urchins

![**Urchin grazing**](images/Urchins.jpg){width=60%} <br>

In the last prac, we worked with this sea urchin dataset. This data was collected marine researchers in BEES who were studying sea urchins at Bare Island near the opening of Botany Bay ([Wright et al. 2005](http://www.int-res.com/abstracts/meps/v298/p143-156/)). Urchins are voracious grazers and at high densities can remove entire kelp forests and leave bare areas known as urchin barrens. 

To study how urchins affect the communities on the rocky reef, researchers set up five experimental treatments on the sea floor:

- cages that enclosed a high density of urchins
- cages that enclosed a low density of urchins
- cages with no urchins (cage control)
- cages with no urchins and no top (open cage)
- control plots with no manipulation.

**These researchers wanted to know: "How do urchin densities affect algal cover over time?"**

The **% cover of `BareRock`** were measured several times over **210 days**, using **quadrats** (Like you did in the Maroubra field trip!)

We fitted a linear model containing **one categorical and one continuous predictor**, this is also know as an **ANCOVA**: 

- `Day` was treated as a covariate. We are not really interested in `Day` as an effect but we know that time is a factor and would likely affect the % of `BareRock`.
- The main effect of `Treatment`. 
- The interaction between Day and Treatment `Day:Treatment`

```{r}
fit_urchins <- lm(logBareRock ~ Day + Treatment + Day:Treatment, data = data_urchins)
```

From this model, we produced this plot: 

<details>
    <summary>Plot code here:</summary>
```{r}
means <- estimate_means(fit_urchins, by = c("Day", "Treatment"))

p1 <-
  ggplot(means, aes(x = Day, y = Mean, color = Treatment)) +
#  geom_ribbon(aes(ymin = CI_low, ymax = CI_high, fill = Treatment), alpha = 0.2) +
  geom_line() +
  labs(
    x = "Day",
    y = "log(BareRock)"
  ) + geom_point(data = data_urchins, aes(y = logBareRock)) +
  facet_wrap(~Treatment) +
  theme_minimal()

```

</details>

```{r, echo=FALSE}
p1
```


> **But we forgot an important sampling design detail!**

The researchers collected data using `Quadrats`, this is the sampling unit. They sampled 360 quadrats. While we're not particularly interested in the `Quadrats` themselves, we know that each time we use them, the % `BareRock` will differ by chance. We also know our 360 `Quadrats` are really a sample from a much wider population of `Quadrats` and we want be able to make inferences about any `Quadrat` in general. 

**Exercise:**

> Your task is to refit the same model, adding `Quadrat` as a random intercept and reproduce the results again ðŸš€!

- Fit a mixed model adding in `Quadrat` a random intercept
- Generate means for `Total` for `Days`, `Treatment` and `Quadrat` 
- Create a plot, fitting lines for each `Quadrat` 

Let's get you hitting the ground running: 

Here is the code to: 

- read in the `data/urchins.csv` dataset. 
- log transform the y variable (`BareRock`) as it is a percentage and will be right skewed. 
- setting the `Quadrat` variable as a factor/catergorical variable as it is recorded as integers.

```{r}
# Read in data 
data_urchins <- read_csv("data/Urchins.csv")  |> 
  mutate(
    logBareRock = log(BareRock + 1),
    Quadrat = as.factor(Quadrat),
  )
```

Now fit the model with the random effect of `Quadrat`. 

```{r}

```

**Questions:**

**1. Compare the output of `parameters(fit_urchins)` and the `paramaters()` output of your mixed model. How did included the random effect of `Quadrat` change the model output?** 

Broadly speaking, the CI of the mixed models are ???. Corresponding, the p values are ???.

**2. What does it mean then if we were to ignore `Quadrat` and not include it as a random effect?**

Ignoring `Quadrat` means that we are not accounting for ??? 

# Random Slopes

## Random slopes in mixed models

![](images/random-effects.png){width=60%}  <br>

A **random slope** allows the effect of a predictor variable to vary across groups in your data.

In a mixed model, a **random intercept** lets each group (like a species, site, or school) have its own baseline value. A **random slope** goes a step further â€” it allows the relationship between a predictor and the outcome to differ between groups.

For example, imagine you're studying how study hours affect test scores across different schools. If you fit a random slope for study hours, you're saying:

> "The effect of studying on test scores might be stronger in some schools and weaker in others."

## Worked Example: Tree height vs leaf area

![](images/trees.jpeg){width=60%}  <br>

In this example, we will use the [**Biomass And Allometry Database (BAAD)**](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/14-1889.1), which contains data on plant structure compiled by Data Dan, using studies from all around the world. You can find more information about the BAAD dataset [here](https://github.com/dfalster/BAAD). This datasets enables us to ask questions about how plant structure varies across species and environments, and how the dimensions of different plant parts (tree height, stem diameter, leaf area, biomass etc) are related to each other.


**Install relevant packages**

To access the data, we will use the `baad.data` R package. To install this, you will need to uncomment and run the following code.

```{r}
# remotes::install_github("richfitz/datastorr")
# remotes::install_github("traitecoevo/baad.data")
```

**Download the data**

```{r, message=FALSE}
# Load library
library(baad.data)

# Download data
baad <- baad_data("1.0.0") # This will take a few minutes

# Look at variable names of the data
names(baad)
```

Let's look at the relationship between **tree height** and **leaf area**.

Data Dan has asked you to: 

- estimate **how much the total leaf area of the tree increases with it's height**, 
- estimate the **likely leaf area of a random tree that is 10m tall**, 

The data in the BAAD dataset contains information on the sizes of tree parts for a number of species at different locations around the world. The data comes from a number of studies. 

The following code prepares the data by: 

- reformatting the data so that it is in the form of a tibble, with columns for the study name, species, location, height, and leaf area. 
- remove rows with missing values for height and leaf area
- added columns for the log10 of height and leaf area.

```{r}
# Dataset assembly
data_trees <- baad$data |>
  as_tibble() |> # Convert dataframe into tibble
  select(studyName, species, location, height_m = h.t, area_leaf_m2 = a.lf) |> # Select some columns and rename them
  filter(!is.na(height_m), !is.na(area_leaf_m2)) # Exclude any NAs in height and area

data_trees 
```

Our first thought is to fit a **simple linear model** to the data. This model **assumes that the relationship between height and leaf area is the same for all species and locations**.

```{r}
fit_simple_raw <- glmmTMB(area_leaf_m2 ~ height_m, data = data_trees)

check_model(fit_simple_raw)
```

However, the data is not well described by a simple linear model. Let's try a **double log10 transformation** to see if that helps.

```{r}
data_trees <- data_trees |> 
  mutate(log10_height_m = log10(height_m), 
         log10_area_leaf_m2 = log10(area_leaf_m2))

fit_simple <- glmmTMB(log10_area_leaf_m2 ~ log10_height_m, data = data_trees)

check_model(fit_simple) # THis might take a few seconds
```

Not perfect, but much better and good enough!

Let's plot the data and the means estimated by this model: 

```{r}
# Estimate means 
means_all <- estimate_expectation(fit_simple, by = c("log10_height_m"))

ggplot(data_trees, aes(x = log10_height_m, y = log10_area_leaf_m2)) +
  geom_point(alpha = 0.3) +
  geom_line(data = means_all, aes(y = Predicted), alpha = 1, col = "orange") +
  geom_ribbon(data = means_all, aes(y = Predicted, ymin = CI_low, ymax = CI_high), fill = "white", linewidth = 3) +
  theme_classic() + 
  labs(x = "Log10 Height (m)",
       y = "Log10 Leaf area (m^2)")
```

What do you see? 

- the line provides a good fit to the data
- the confidence intervals around the mean are **narrow**, so narrow that we can barely see them! This indicates that the model is confident in its predictions about the mean relationship.
- however...there's a lot of **unexplained variation** in the data (above and below the line). So any given tree is likely could have a leaf area that is quite different from the mean.

In fact, for a tree of 10m tall plant (log10 = 1), you could see a leaf area anywhere from 0.2 to 2.4 log10 units. Back transforming this gives a range of m2 leaf from 
```{r}
10^c(0.2, 2.4)
```

This is a huge range! Surely we can do better!

## Linear model with random effect by species

We could try to improve the model by **allowing the relationship between height and leaf area to vary between species**. We can hypothesise that **some of the variation in the relationship** between height and leaf area **is due to differences between species**. We can account for this by fitting a model with a **random intercept AND slope** for species.

```{r}
# Fit a model with random slopes for species 
fit_rs <- glmmTMB(log10_area_leaf_m2 ~ log10_height_m + (1 + log10_height_m | species), data = data_trees)
```

In the code above, the model terms:

- `(1 + log10_height_m | species)` specifies that we want to fit a model with a **random intercept and slope** for species. This means that the **relationship between height and leaf area can vary between species**. 
- The `| species` term specifies that the **random effects** are occurring at the species level.

> Let's plot this mixed model fit and compare it to our first simple model (without any random effects).

```{r}
# Estimate the predictiosn for the model for height x species
means_sp <- estimate_expectation(fit_rs)

# Estimate marginal means for the model by height, averaging out species
# This might take a while to compute
means_all <- estimate_expectation(fit_rs, by = c("log10_height_m"))

# Make the plot
ggplot(data_trees, aes(x = log10_height_m, y = log10_area_leaf_m2, color = species, group = species)) +
  geom_point(alpha = 0.3, show.legend = FALSE) +
  geom_line(data = means_sp, aes(y = Predicted), alpha = 0.5, linewidth = 1) +
  geom_line(data = means_all, aes(y = Predicted, group = 1),
            linewidth = 2, colour = "lightgrey") +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "Log10 Height (m)",
       y = "Log10 Leaf area (m^2)")
```

Wow! The model with random slopes for species provides a much better fit to the data.

- Each coloured line gives the **relationship between height and leaf area for a different species**. 
- The grey line gives the **average relationship across all species**.

Just as we thought, the relationship between height and leaf area is not the same for all species and there is a lot of spread between species! 
The mixed model is able to capture the variation in the relationship between height and leaf area between species. 

We can assess whether the variation in the data due to the random effect is signifcant by comparing the mixed model to the simple model. We can do this using an `anova()` test , which will tell us if the random slopes model is significantly better than the simple model. It is!

```{r}
anova(fit_simple, fit_rs)
```

(If we wanted, we could also test whether the random slopes model is significantly better than a model with just a random intercept for species. This would tell us if the relationship between height and leaf area varies between species, as well as the average leaf area for each species.)

## Quantifying the variance components

Now that we have a richer model, we can find out how much of the variation in the data is due to: 

- the fixed effects
- the random effects
- residual noise.

We can quantify these via the **R^2^ of models, with and without the random effects**. 

> Recall that R^2^ describes the proportion of variation explained by our model and ranges between 0 - 1. An R^2^ value of 1 means our model describes the variation in the data perfectly. 

Let's get some numbers first using the `performance()` function
```{r}
# Calculate the r2 values
performance(fit_rs)
```

In the output:

- `R2 (marg.)` ( Marginal R^2^ ) is the proportion of the total variance that is **explained by the fixed effects alone**. 
- `R2 (cond.)` ( Conditional R^2^ ) is the proportion of the total variance that is explained by the **fixed and random effects together**.

- So, `R2 (cond.)` - `R2 (marg.)` is the proportion of the total variance that is explained by the **random effects alone.**
- And, 1 - `R2 (cond.)` is the proportion of the total **variance that remains unexplained**, i.e. the residuals. 

So we can see that: 

- `log10 height` explains **78.8%** of the variance in `log10 leaf area` `R2 (marg.)` 
- Adding species as random effects explains **an additional 17%**, bringing the total explained variance to **96.1%**. `R2 (cond.)` - `R2 (marg.)`
- This leaves only **3.9%** of the variance unexplained 1 - `R2 (cond.)`

## Comparing the models

Finally, let's compare the slope estimates and predictions from the random slopes model to the simple model

**Slopes**

```{r}
parameters(fit_simple)
parameters(fit_rs)
```

- The simple model gives a slope of **1.71**, the mixed model gives a slope of **1.79**. 
- The confidence intervals for the **simple model are much narrower** than the random slopes model. And they don't overlap the slope estimate from the mixed model model! 

**Predictions**

Let's use both these models to estimate the leaf area of a tree that is 10m tall. We need to create some placeholder data for `estimate_expectation()` so it can make this prediction. 

```{r}
# Create a dataset to pass in to predict output
data_pred <- tibble(
  log10_height_m = log10(10))

# Make predictions from the models
estimate_expectation(fit_simple, data = data_pred)
estimate_expectation(fit_rs, data = data_pred)
```

- The simple model is predicting a higher value with very narrow confidence intervals. 

- The random slopes model is predicting a lower value with much wider confidence intervals.

## Summary

In this example, we have seen how to fit a mixed model with random slopes and intercepts for species. Compared to the simple model, the random slopes model: 

- allows the **relationship between height and leaf area to vary between species** 
- **captures more total variation** in the data and shows us that this variation due to differences between species,
- **estimates a steeper slope** for the relationship between height and leaf area,
- provides a **more conservative estimate (wider CI)** for the leaf area of a random tree that is 10m tall, due to differences between species,
- once species is known, provides a tighter relationship between height and leaf area.

We can conclude that: 

- the simple model is **overly confident in its predictions**, indicated by the narrow confidence intervals.

It doesn't know that species can differ a lot from each other. This suggests less leaf area gained per unit height.

> Using the mixed model gives us a much more nuanced view on the data!

## Your turn: Stem diameter vs biomass

![](images/stem.jpeg){width=60%}  <br>

Another potential use of data from the BAAD is to quantify the **relationship between stem diameter and biomass**. 

There is a lot of ineterest in estimate the amount of carbon stored in forests, and using growing trees to remove carbon from the atmosphere. Since biomass is mostly (~42%) carbon, we can use the biomass of a tree as a proxy for the amount of carbon stored in the tree.

**Exercise:**

Your task is to fit a model between stem diameter and biomass, with random slopes for species, and use this to estimate: 

- rate at which `log10_biomass` increases with `log10_stem_diameter`, and
- the `log10_biomass` of a tree with a stem diameter of 20cm.

Here's the code to get you started with the data:

```{r}
# dataset assembly
data_trees <- baad$data |>
  as_tibble() |> # Treat dataframe as a tibble
  select(studyName, species, location, stem_diameter_m = d.ba, height = h.t, biomass_kg = m.so) |> # Select and rename some columns
  filter(!is.na(stem_diameter_m), !is.na(biomass_kg)) |> # Exclude NA in  and biomass_kg
  mutate(
    log10_stem_diameter_m = log10(stem_diameter_m), # log10 transform 
    log10_height = log10(height), log10_biomass_kg = log10(biomass_kg) # log10 transform 
  )

data_trees
```

Now give it a go! Unlike the example above, you don't need to fit the simple model first. Just go straight to fitting the mixed model with random slopes for species model. 

```{r}

```

**Questions:**

**1. What is the rate at which `log10_biomass` increases with `log10_stem_diameter_m`?** ???

**2. What does the conditional R^2^ capture?**

the ???

**3. How much variation is explained by only the random effects in your model? _Hint: Using performance(), compare the marginal and conditional R^2^_**

???

**4. What is the `log10_biomass` of a tree with a stem diameter of 20cm e.g `log10_stem_diameter_m = log10(0.2)`?**

???

