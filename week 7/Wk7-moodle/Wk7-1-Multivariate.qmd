---
title: "Week 7-1 Multivariate methods"
format: 
  html:    
    self-contained: true
editor_options: 
  chunk_output_type: console
html-math-method: mathml
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set the working directory
rprojroot::has_file("BEES2041-code.Rproj") |>
  rprojroot::find_root() |>
  file.path("week 7/Wk7-moodle") |>
  setwd()

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)
library(kableExtra)
```

## Introduction

This week's practicals provide an introduction to some of the graphical and analytical techniques used for multivariate data sets (those with more than one dependent variable).

Today we we will focus on two graphical techniques: multidimensional scaling (MDS) and cluster cluster diagrams, and some of the issues that need deciding on before making these plots (transformations and choices of similarity measures).

![](images/mds.jpg){width="50%"}

## Key learning objectives

At the end of today's practical, you should be able to:

-   Calculate a variety of similarity/dissimilarity coefficients to represent the differences among samples (or variables) in multivariate data sets
-   Create and interpret an multi-dimenional scaling (MDS) plot
-   Create and interpret cluster diagrams to summarise the relationships among samples or variables
-   Understand that data may need to be transformed or standardised before making graphs
-   Understand the logic behind permutational MANOVA
-   Define a principal component
-   Interpret an ordination from a principal components analysis (PCA)

Letâ€™s dive in! ðŸš€

## Setting up: Materials

Everything you need for this prac is on Moodle

1.  Download this week's materials zip file from Moodle, from the course page
2.  Unzip the file by:

-   MacOS: Double clicking the zipfile
-   Windows: Right click on the zip file and click "Extract All"

3.  Move the extracted folder into the folder where you store materials for `BEES2041/`
4.  **Click on the Rstudio project file, eg. `Wk7-Multivariate.Rproj`** to open the RStudio project and you're in!!!

We will be working with various datasets collected. These are in the folder `data/`.

You will work in the relevant Quarto document for this prac. Within each Quarto doc there are several challenges for you to complete on your own devices in order to **answer the questions on Moodle**.

## Setting up: Packages

> **Note** that when running R in the browser we need to install the packages each time we start a new session or after you've checked your answers. We're only going to install the parts of `tidyverse` we need for this prac.

We'll also be using a new package, called `vegan`, which includes a range of tools for visualising and analysing community data in ecology, but useful for all sorts of multivariate data sets.

```{r, eval=FALSE}
install.packages("tidyverse")
install.packages("readr")
install.packages("ggplot2")
install.packages("vegan")
```

> Remember to load the packages into R to use em!

```{r, results='hide', warning=FALSE, message=FALSE}
library(dplyr)
library(readr)
library(ggplot2)
library(vegan)
```

# 1) Similarity coefficients

Many techniques for visualising and analysing multivariate data start with **calculating the similarity between two samples**, based on the values for all variables measured from those samples.

There are many ways to do this, but if you understand a few of them you can get the general idea that the **similarity (or difference) between two samples can be represented by a single number (a value of a similarity coefficient)**.

![](images/Beach.jpeg){width=70%}

Let's take a look at this **multivariate dataset** collected by a physical geographer who was contrasting beach profiles at three beaches around a coral atoll.

Using the same methods at each site, the following data were collected from each beach:

```{r, echo=FALSE}
Beaches <- read_csv(file = "data/Beaches.csv")

Beaches |>
  kbl() |>
  kable_styling(bootstrap_options = "striped")
```

## 1a) What are the samples and what are the variables?

**Questions:**

- What are the samples? `r cloze("the beaches", c("the beaches", "sediment sizes","current speeds", "wave height", "slope"))`

- What are the variables? `r cloze("the measures taken from each beach", c("the measures taken from each beach", "the beaches"))`

First, let's load the data into R

```{r}
Beaches <- read_csv(file = "data/Beaches.csv")
```

## 1b) Summarise the relationships among samples using the Pearson correlation coefficient

To obtain the **Pearson correlation coefficient**:

-   Use the `cor()` function

```{r}
cor(Beaches)
```

This is a **similarity matrix* *that summarises the **relationships among beaches**.

**Questions:**

-   **What is the similarity between Beach 1 and 3 as defined by the Pearson correlation coefficient?** `r cloze("0.8935143", c("1.0000000","0.9940330","0.8935143","0.8474139"))`

-   **What is the similarity between Beach 2 and 3 as defined by the Pearson correlation coefficient?** `r cloze("0.8474139", c("1.0000000","0.9940330","0.8935143","0.8474139"))`

-   **What is the value of the coefficient if two samples are identical?** `r cloze("1.0000000", c("1.0000000","0.9940330","0.8935143","0.8474139"))`

## 1c) Summarise the relationships among samples using Euclidean distance

The Euclidean distance for each pair of samples is defined as:

$$
d(y_1,y_2) = \sum_{j=1}^{p}\sqrt{(y_{1j} - y_{2j})^2}
$$


$$
d(y_1,y_2) = \sum_{j=1}^{p}\sqrt{(y_1_j - y_2_j)^2}â€º
$$

where $y_1$ and $y_2$ are the values of variable $j$ in sample 1 and 2; $p$ is  the number of variables

-   Use the `dist()` function with `method = "euclidean", diag = TRUE`
-   You need the samples as rows for distance analysis so make sure you transpose the matrix first, using the `t()` function, i.e. `(t(Beaches))`

```{r}
dist(t(Beaches), method = "euclidean", diag = TRUE)
```

This is also a **similarity matrix**, but now using a different measure (**Euclidean distance**) to quantify the similarities among beaches.

**Questions:**

- What is the similarity between Beach 1 and 2 as defined by Euclidean difference? 
`r cloze("5.321128", c("0.000000","5.321128","9.582881","4.314673"))`

- What is the value of the coefficient if two samples are identical? 
`r cloze("0.000000", c("0.000000","5.321128","9.582881","4.314673"))`

# 2) Heavy metals in sediments

![](images/harbour.jpg){width=70%}

Twelve sites in an estuary were sampled for the concentrations of eight heavy metals [(Clarke and Warwick, 1994)](https://www.researchgate.net/profile/Aimeric-Blaud/post/Can_anyone_help_me_in_understanding_and_clearly_interpreting_ANOSIM_Analysis_of_Similarityand_SIMPER_Similarity_percentage_analysisresults/attachment/59d63f3ec49f478072ea9897/AS%3A273777156395016%401442284968397/download/PRIMER-E.pdf). The sites were aligned along a linear transect with the middle of the transect closest to a sewage dumping area (i.e., 1 and 12 furthest from dump, 6 closest to the dump).

We will measure the similarity among sites, then use an **ordination plot** to display the **relationships among samples**.

Read the data set, â€œHeavyMetals.csvâ€, into R and have a look at itâ€™s structure (samples as rows, variables as columns).

```{r}
HeavyMetals <- read_csv(file = "data/HeavyMetals.csv")  
```

The first column is the name of each site. We donâ€™t want to use the site numbers in the analysis, so need to make a data frame with just the metal variables.

```{r}
HeavyMetals_vars <- select(HeavyMetals, -Site)
```

Before we calculate the similarity among sites, we will square root transform the data. This will reduce the influence of very large values (more on the reasons for transformations next practical).

```{r}
HeavyMetals.sq <- sqrt(HeavyMetals_vars)
```

Now we can use the `dist` function again to calculate to create the similarity matrix, using the Euclidean distance as our similarity coefficient.

```{r}
HeavyMetals.dis <- dist(HeavyMetals.sq, method = "euclidean")
```

The numbers in this similarity matrix are then used to create a multi-dimensional scaling plot to visualise the differences among sites. These are introduced in the lectures and you can also read the short tutorial on [Environmental Computing](https://environmentalcomputing.net/graphics/multivariate-vis/mds/).

We use the function `metaMDS` from the package vegan.

```{r}
HeavyMetals.mds <- metaMDS(HeavyMetals.dis, autotransform = FALSE, trace=FALSE)
```

The `autotransform` argument tells the function not to further transform the data, and the `trace` argument stops it giving us all the details of the process trying to find the best solution.

To view the ordination, we can just plot the object we just created.

```{r}
plot(HeavyMetals.mds)
```

While this shows each site as a dot and their relationships, it isn't much use yet because we don't know which site is which. To get a nicer plot, we can extract the x and y coordinates from the MDS into a new data frame, add labels for each site, and plot with ggplot.

```{r}
Metals_xy <- data.frame(HeavyMetals.mds$points)

Metals_xy$Site <- HeavyMetals$Site

ggplot(Metals_xy, aes(MDS1, MDS2)) + geom_text(aes(label=Site))
```

Before we interpret the MDS plot, we should check the stress value.

```{r}
HeavyMetals.mds$stress
```

## 2a) What does the stress value represent?

`r cloze("a measure of how well the distance between samples on the plot corresponds with the actual multivariate distance", c("a measure of how well the distance between samples on the plot corresponds with the actual multivariate distance", "a measure of how well the variables are correlated", "a measure of whether the data are normally distributed", "a measure of how close together all the samples are on the plot", "a statistical test contrasting samples in 2D space"))`

If you are satisfied that the plot effectively visualises the similarity among samples, go ahead and interpret the plot with the very simple rule that those points that are closest together are the most similar.

## 2b) Which site is least similar to any other site?

`r cloze("S1", c("S1", "S9","S6", "S2"))`

## 2c) Can you see any patterns in the similarity among sites that you can relate to the sampling design? Which sites are most similar to the dump site? {type="essay"}

# 3) Nutrient enrichment experiment

![](images/53711279_1967294661.png){width="70%"}

The effect of nutrient enhancement on the composition of weed species on a farm was investigated. Ten replicate plots were established for each of three treatments: control (no nutrients), low nutrient dose added, and high nutrient dose added. After 12 weeks, all weed seedlings were identified and counted. Twelve species of weeds were recorded.

Produce a MDS plot to display the relationships among samples.

First, read in the datafile â€œNutrientEnrichment.csvâ€

```{r}
Nutrients <- read_csv(file = "data/NutrientEnrichment.csv")
```

As for the metals data set, we first need to select just the columns that have dependent variables (the abundance of each weed). The first two columns are treatment labels that we will use for plotting. We can use `contains` within the `select` function to pull out all columns that include Weed in the column name.

```{r}
Nutrients_vars <- select(Nutrients, contains("Weed"))
```

We can then square root transform the data to reduce the influence of very large values (more on the reasons for transformations later).

```{r}
Nutrients.sq <- sqrt(Nutrients_vars)
```

Instead of creating a similarity matrix and doing the MDS in two steps as you did for the heavy metals data, the function `metaMDS` is able to compute dissimilarities and perform MDS directly from the data frame that holds just the variables of interest.

```{r}
Nutrients.mds <- metaMDS(Nutrients.sq, distance = "bray", autotransform = FALSE, trace=FALSE) 
```

We can plot the ordination as you did previously for the heavy metals data set by extracting the x and y coordinates of the points, adding the treatment labels from the original data set, and plotting with each point labeled by treatment.

```{r}
Nutrients_xy <- data.frame(Nutrients.mds$points)

Nutrients_xy$Treatment <- Nutrients$Treatment

ggplot(Nutrients_xy, aes(MDS1, MDS2, color = Treatment)) + geom_point()
```

##3a) Does the ordination distinguish between experimental treatments? Describe the patterns in the plot {type=essay}

# 4) Species composition in rock pools

![](images/rock_pool.PNG){width="70%"}

On the Maroubra field trip, multivariate data were collected that contrasted the species composition of the intertidal community living in rock pools to neighbouring rocky habitats that were that were emergent at low tide.

We can use a MDS plot to visualise differences in species composition between the samples taken.

Using the same methods as for the previous questions, produce a MDS plot to visualise differences in species composition between habitats (rockpool vs emergent).

Read the Maroubra data into into R.

```{r}
Rock_pools <- read_csv(file = "Rock_pools_2025.csv")
```

Look at the data frame so that you are familiar with the structure of the data collected and know what are the samples, what are the variables and how the samples are grouped by the habitat treatment.

The first column holds a unique identifier for each quadrat sampled (the combination of data sheet and quadrat on that sheet).

These data have been entered by your friendly demonstrators in a long format, with species as a single variable and their abundance values in a second column. To get this ready for multivariate analyses we need to rearrange this data so that each species is a separate variable that holds the abundance values.

We can use the tidyverse function `pivot.wider` to rearrange this data set into a wide format.

```{r}
Rock_pools_wide <- Rock_pools |>
 pivot_wider(id_cols = 1:2, 
 names_from = species,
 values_from = abundance,
 values_fill=0)
```

-   `id_cols` defines which variables uniquely identify each sample (in our case, the first two columns with quadrat ID and habitat)

-   `names_from` is the variables that holds all the values that we want to become new variables

-   `values_from` is the variable that holds the all the data that will fill in each new variable

-   `values_fill` allows us to put in a zero for any missing combinations (i.e., if a species wasn't listed in a quadrat it will fill in a zero).

Next, we need to remove the first two columns to end up with a data frame that just has the variables for each species.

```{r}
Rock_pools_vars <- select(Rock_pools_wide, -Quadrat_ID, -habitat)
```

Some of the variables are % cover (the red, brown and green algae) and some are counts of individual animals. Clearly 100% cover is not equivalent to 100 snails, so we can standardise these variables to ensure each variable is on the same scale.

We can use the function `decostand` from vegan to standardise each variable by the maximum value of that variable. We then end up with all variables having values between 0 and 1. No need to do any other transformations.

```{r}
Rock_pools_vars_std <- decostand(Rock_pools_vars, method= "max")
```

Using the same methods as for the previous questions, produce a MDS plot to visualise differences in species composition between habitats.

##4a) Does the ordination plot suggest any differences in species composition between rock pools and neighbouring emergent habitats? How confident are you that the MDS plot effectively represents the relationships among all samples in the multivariate data set? {type=essay}

# 5) Heavy metals in sediments (cluster analyses)

![](images/harbour.jpg){width="70%"}

You first explored the patterns in heavy metal pollution in this data set with a MDS ordination plot. To visualise the similarity among sites with a cluster diagram, we can use the `hclust` function.

To do this for the heavy metal data set, we would use:

```{r}
HeavyMetals.cluster <- hclust(dist(HeavyMetals.sq, method = "euclidean"), method = "single")
```

where HeavyMetal.sq is the square root transformed data we created earlier. `method = "euclidean"` in the `dist` function specifies the choice of similarity coefficient and `method = "single"` specifies the choice of linkage method.

Finally plot the object that was created by the `hclust` function.

```{r}
plot(HeavyMetals.cluster)
```

We can make this a bit neater by plotting it as a dendrogram with all the samples lined up along the bottom.

```{r}
plot(as.dendrogram(HeavyMetals.cluster))
```

##5a) Which two samples (i.e., sampling sites) are most similar to one another `r cloze("5 and 8", c("5 and 8", "10 and 11","1 and 6", "2 and 12"))`

##5b) Does the cluster analysis suggest any effects of the sewage dump? State reasons. {type=essay}

We can also use a cluster diagram to display the relationships among variables.

Methods as above except you need to make the heavy metals as rows and the stations as columns before running the cluster analysis (use the transpose function, `t`).

```{r}
HeavyMetals.cluster.trans <- hclust(dist(t(HeavyMetals_vars), method = "euclidean"), method = "single")

plot(as.dendrogram(HeavyMetals.cluster.trans))
```

## 5c) The concentrations of which two metals are most similar to each other in terms of euclidean distance? `r cloze("Co and Cd", c("Co and Cd", "Mn and Zn","Cu and Pb", "Ni and Co"))`

# 6) Choosing whether to transform or standardise the data

Multivariate data sets are often transformed prior to making plots. This is not to meet the assumptions of a statistical test (these plots are not hypothesis tests), but to vary the relative importance of variables that may be measured on different scales.

In the heavy metals data set, the concentration of the metals vary widely with manganese having values up to 2470 while cobalt has values no higher than 15. Consequently, the similarity between samples using Euclidean distance will be very strongly influenced by manganese values and those metals with low concentrations will have little influence.

If you wanted to create a plot to explore the composition of contaminants, not just their absolute concentrations, then it is a good idea to standardise the variables before creating any cluster diagrams. There are several ways to do this (e.g., divide by maximum value, divide by mean etc).

Here, we will standardise each metal variable to a new version with a mean of zero and variance of 1. `decostand` comes from the package vegan, so remember to load that with `library(vegan)`.

```{r}
HeavyMetals.standardised <- decostand(HeavyMetals_vars, method = "standardize")
```

Any plots we create with this standardised data will have all metals having a similar influence on the similarity among samples and not be strongly influenced by just those variables with the highest concentrations.

With the methods from above, create a cluster diagram to display the relationships among sites based on standardised metal concentrations.

```{r}
plot(as.dendrogram(hclust(dist(HeavyMetals.standardised, method = "euclidean"), method = "single")))
```

## 6a) Make some notes on how the relationships between sites are altered by standardising the variables. {type="essay"}

For contrasts of species composition in community data sets (i.e., the abundance of many species in many samples) it is common to square root or log transform the abundance data prior to multivariate analyses. A more extreme standardisation is to convert the abundance data to presence/absence (i.e, 0 or 1) only.
