---
title: "Week 7-1 Multivariate methods"
format: 
  html:    
    self-contained: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set the working directory
rprojroot::has_file("BEES2041-code.Rproj") |>
  rprojroot::find_root() |>
  file.path("week 7/Wk7-moodle") |>
  setwd()

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)
```

## Introduction

In this practical, we will continue our look at methods to analyses and visualise complex, multivariate data sets - ones where more than one response variable has been measured from each replicate sample.

First, we will learn how to run a formal hypothesis test on multivariate data (permutational multivariate analysis of variance). This will be used to contrast species composition in and out of rock pools from the class data set collected at Maroubra.

Second, we will get an introduction to principal components analysis (PCA), a widely used technique to visualise multivariate data.

![](images/nature14140-sf2.jpg){width="70%"}

For some questions, we will be using the data sets from last practical. To avoid a lot of work, add today's code to the notebook you created for the first multivariate practical rather than make a new one. You can then easily re-run previous code to import data and give you some of the objects in R that you will need for today's work.

## Key learning objectives

At the end of today's practicals, you should be able to:

-   Understand the logic behind permutational MANOVA
-   Understand the aims of principal components analysis (PCA)
-   Interpret an ordination from a principal components analysis (PCA)
-   Explore the output from PCA to understand relationships among variables

Let’s dive in! 🚀

## Setting up: Packages

For some questions, we will be using the data sets from last practical. To avoid a lot of work, add today's code to the notebook you created for the first multivariate practical rather than make a new one. You can then easily re-run previous code to import data and give you some of the objects in R that you will need for today's work.

We'll again be using the package, called `vegan`, so ensure this is loaded.

```{r, eval=FALSE}
install.packages("dplyr")
install.packages("readr")
install.packages("ggplot2")
install.packages("vegan")
```

```{r, results='hide', warning=FALSE, message=FALSE}
library(dplyr)
library(readr)
library(ggplot2)
library(vegan)
```

```{r}
Nutrients <- read.csv(file = "data/NutrientEnrichment.csv", header = TRUE)
Nutrients_vars <- select(Nutrients, contains("Weed"))
Nutrients.sq <- sqrt(Nutrients_vars)
```

# 7) Nutrient enrichment experiment (multivariate hypothesis testing)

![](images/53711279_1967294661.png){width="70%"}

So far, we have been making plots to visualise differences among samples with multiple variables measured per sample, but have not run any formal statistical tests to determine whether any differences we found are likely to be due to chance sampling or not.

There is a whole world of multivariate statistical tests, but we will show you just a couple to demonstrate how you can test hypotheses with multivariate data.

In the nutrient enrichment experiment, we had three treatments (control, low and high nutrients) and 12 response variables (the abundance of each weed species). If we had measured just one variable, we would use a linear model (analysis of variance) to contrast that variable among treatments. With many variables measured, we can use multivariate analyses of variance based on the similarity matrix.

Instead of partitioning the variation in the actual values of the variables (i.e., abundance of our each of the weed species), this analysis partitions the variation in the values of the similarity matrix among and within treatment groups.

A good way to think of this is that if there was a strong effect of treatment, then all the replicates within each level of treatment are more likely to be similar to each other than they would be to replicates from another level (i.e., within group similarities higher than among-group similarities).

We will use the function `adonis2` in vegan:

```{r}
adonis2(Nutrients_vars ~ Nutrients$Treatment, permutations = 999, method = "bray")
```

Like with the previous model formulae you have seen in R, the dependent variables are left of the `~` symbol, and the independent variable are on the right (i.e. `Y ~ X` is aimed to predict Y from X). `method = "bray"` tells vegan to use the Bray-Curtis index to represent the similarity between samples, recommended for species-abundance data.

The output should look pretty familiar to you - very similar to an ANOVA table.

## 7a) Is there evidence to reject the null hypothesis?

`r cloze("yes, the P value is less than 0.05", c("yes, the P value is less than 0.05", "yes, the P-value is greater than 0.05", "no, the P-value is 0.001", "yes, the F value is greater than 1","no, the F value is greater than 9"))`

There are some important differences from the analyses of variance you ran with univariate data (one dependent variable).

-   the analysis ran on similarities among samples, not the actual values of the original variables

-   the probability of the test statistic produced (pseudo-F) cannot be obtained from the traditional probability distribution of F. The probabilities here are obtained by permutation. From 999 permutations of the original data versus the treatment labels, it calculates how often a value of pseudo-F as large as yours would occur.

We can save the output from `adonis2` and use a density plot to see just how likely our value of pseudo-F was.

```{r}
Nutrients_test <- adonis2(Nutrients_vars ~ Nutrients$Treatment, permutations = 999, method = "bray")

densityplot(permustats(Nutrients_test))
```

This produces a frequency histogram of pseudo-F obtained from the 999 permutations of the raw data. This is best viewed as what would happen if the null hypothesis was true. The value you obtained is noted with the vertical black line.

You should have output that shows that this null hypothesis is rejected - there is some difference in weed species composition among all three treatments. A next step is to try and understand which treatment differed from which. For various reasons, unlike some commercial software packages, the authors of vegan do not provide code for doing pairwise tests.

A work around is to run separate pairwise tests using subsets of data with only two levels of a factor. The following code creates subsets of rows for each comparison (note that `!=` means “not equals to”“) and runs the permutational MANOVA again with just the rows with the weed variables.

Remember that `filter` is the function for subsetting rows, and `select` is the function for subsetting columns - both from dplyr (need to load that package if you haven't for your current session).

```{r}
Nutrients_ControlvsLow <- filter(Nutrients, Treatment != "High")
Nutrients_ControlvsLow_vars <- select(Nutrients_ControlvsLow, contains("Weed"))

adonis2(Nutrients_ControlvsLow_vars ~ Nutrients_ControlvsLow$Treatment, permutations = 999, method = "bray")
```

```{r}
Nutrients_ControlvsHigh <- filter(Nutrients, Treatment != "Low")
Nutrients_ControlvsHigh_vars <- select(Nutrients_ControlvsHigh, contains("Weed"))

adonis2(Nutrients_ControlvsHigh_vars ~ Nutrients_ControlvsHigh$Treatment, permutations = 999, method = "bray")
```

```{r}
Nutrients_LowvsHigh <- filter(Nutrients, Treatment != "Control")
Nutrients_LowvsHigh_vars <- select(Nutrients_LowvsHigh, contains("Weed"))

adonis2(Nutrients_LowvsHigh_vars ~ Nutrients_LowvsHigh$Treatment, permutations = 999, method = "bray")
```

This approach has the problem of increasing the the risk of type 1 error with increasing numbers of tests being run. The simplest correction for this is called the Bonferonni correction, where the significance level is adjusted by the number of hypotheses being tested. In this example, you would multiply each P value by three before interpreting which comparisons are significant (in other words, α for each test becomes 0.05/3).

## 7b) What do the multivariate tests and their associated probabilities tell you about the differences in species composition among nutrient treatments?

Use the pair-wise tests to establish which treatments differ from which. {type="essay"}


# 8) Species composition in rock pools (multivariate hypothesis testing)

![](images/rock_pool.PNG){width="70%"}

For your practical report, you want to accompany the text and figures that contrast species composition in and out of rockpools, with a formal statistical test. You can use a permutational multivariate analysis of variance for this, modifying code that you just used for the nutrient enrichment experiment.


# 9) Advanced statistics content!

There is considerable current debate in the statistical literature about the best ways to analyse multivariate data sets like the one you just analysed. Analyses that partition variation in the similarity metrics have been criticised for:

-   not accounting for the inevitable problems with homogeneity of variance that are present in environmental data sets. This means that highly abundant species have a very large influence.

-   lacking an underlying model (i.e. you are not estimating any parameters from your data). This means that there aren't tests for interactions in a traditional sense, and normal diagnostics (like residual vs fitted plots) are not available. This is a problem because you can't check whether any transformations have actually improved your data.

Research at UNSW by [Professor David Warton's group](http://www.eco-stats.unsw.edu.au/) is focussed on developing methods to better analyse these sorts of data, and they have released an R package 'mvabund' that uses model-based approaches to analyse multivariate data sets. David has even promoted his package with a [video](https://youtu.be/KnPkH6d89l4) drawing on the considerable talents of 80's pop star Rick Astley!

![](images/rick-astley.jpg){width="150"} ![](images/mvabund_hex.png){width="150"}

# 10) Principal components analysis

PCA is a commonly used method to visualise data with many variables. We will use it to analyse differences in the shape of leaves.

A plant physiologist is attempting to quantify differences in leaf shape between two species of tree. She recorded the following five measurements from ten leaves of each species:

![](images/leaf.png)

Read in the data from “Leafshape.csv”:

```{r}
Leaf_shape <- read.csv(file = "data/Leafshape.csv", header = TRUE)
```

First, have a look at the relationships among all five variables. Select just the columns with the length data then use the `pairs` function to make a series of scatter plots.

```{r}
Leaf_shape_vars <- select(Leaf_shape, -Species)

pairs(Leaf_shape_vars) 
```

These relationships among variables can also be summarised numerically, by calculating the similarity matrix using the Pearson correlation coefficient.

Check the correlations between the all five leaf measurements.

```{r}
cor(Leaf_shape_vars)
```

We obviously can’t plot these data in five dimensions, but if you look at just three of the five variables, you can get a sense of what PCA is trying to do - fit a new X-Y plane as close as possible to all points in 5 dimensional space.

![](images/Test2.gif)

## 10a) What makes this data set particularly suitable for PCA?

`r cloze("there are multiple variables that are correlated with each other", c("there are multiple variables that are correlated with each other","there are lots of replicate samples","there are multiple variables measured","there two groups of samples that we want to contrast"))`

To conduct a principal components analysis on the leaf data, we can use the `princomp` function in base stats.

```{r}
leaf.pca <- princomp(Leaf_shape_vars, cor = FALSE)
```

`cor. = FALSE` indicates you want to use the covariance matrix, which is appropriate when all variables are measured on the same scale. Change to `cor = TRUE` to scale the variables and use a correlation matrix (recommended when all the variables are on different scales).

Principal components analysis produces a lot of graphical and numerical output, which can explore by further examining the object (leaf.pca) we just created.

## Producing a score plot

This is the ordination of all 20 leaf samples in the new two-dimensional space defined by principal component 1 and 2.

```{r}
plot(leaf.pca$scores, pch = 16)
```

You can also produce a biplot with the relationships between the original variables and the principal components overlaid on the score plot.

```{r}
biplot(leaf.pca)
```

## Understanding how much variance is explained by each component

PCA is aiming to represent as much variance as possible in the first few principal components. We can check whether it is doing a good job by exploring the summary of the `leaf.pca` object.

```{r}
summary(leaf.pca)
```

## 10b) Enter the proportion explained by the first two principal components

PC1 `r cloze_numerical(0.83, tolerance = 0.01)` PC2 `r cloze_numerical(0.22, tolerance = 0.01)`

These are also visualised by a **scree plot**. Note that the Y-axis presented is not the % of variation explained.

```{r}
screeplot(leaf.pca, type = 'lines')
```

## Understanding how the original variables relate to the principal components

We gain further insight into the relationships among variables and help with interpreting our plot by examining the strength of the correlations between the original variables and the principal components.

These relationships can be seen in the biplot (above) but also obtained numerically by extracting the loadings from the object produced by the PCA. Note that this output does not print very small values, so the empty cells can be considered close to zero (i.e., no correlation).

```{r}
loadings(leaf.pca)
```

## 10c) Which of the original variables is most strongly correlated with principal component 1?

`r cloze("Width 1", c("Width 1", "Width 2","Length 1","Length 2","Petiole length"))`

## Producing a more effective plot to visualise the data

Once you understand those issues, you want to produce a plot that addresses the question first posed by the researcher. This was to distinguish the shape of leaves from two species. We can do this by colour-coding the samples on the score plot.

Extract the x and y coordinates from the pca object into a data frame.

```{r}
Leaf_xy <- data.frame(leaf.pca$scores)
```

Add the labels for the two species to this data

```{r}
Leaf_xy$Species <- Leaf_shape$Species
```

Create a scatter plot with points color coded by species.

```{r}
ggplot(Leaf_xy, aes(Comp.1, Comp.2, color = Species)) + geom_point()
```

## 10d) Does the principal components analysis suggest any size or shape differences between the leaves of the two species? {type="essay"}

## 10e) Using the coefficients and the loadings, which variables are predominantly associated with the separation of species A and B.

`r cloze("Width 1 and Width 2", c("Width 1 and Width 2", "Width 1 and Total length", "Total length and Petiole length", "Only Width 1"))`

# 11) Heavy metals in sediments (PCA)

![](images/harbour.jpg){width="70%"}

Using the heavy metal data, we can compare the output from cluster analysis, principal components analysis and multidimensional scaling.

Modify your previous code to conduct a PCA on the heavy metal data. Given that the variables were measured on very different scales, either use a standardised data set and `cor= FALSE` in the `princomp` function or use `cor = TRUE` to run the PCA with a correlation (not covariance) matrix.

```{r solution}
metals.pca <- princomp(HeavyMetals_vars, cor = TRUE)
loadings(metals.pca)
```

## 11a) Which variable (i.e., metal) is most strongly correlated with the values of PC1?

`r cloze("Cr", c("Cr","Cu","Mn","Co","Ni","Zn","Cd","Pb","Cr"))`

##11b) Comparing all your classification or ordination plots, discuss which technique best visualises the patterns in metal contamination among sites. Do any clearly identify sites close the sewage dump? {type="essay"}


# 12) Cave hydrology

![](images/caves.jpg){width="70%"}

Professor Andy Baker (School of BEES, UNSW) works in caves to better understand the history of Australia's climate and how climate affects valuable groundwater resources. He looks at the geochemistry of stalagmites to reconstruct climate histories and impacts of fire, and flows of water underground to understand recharge of groundwater (i.e., how do rainfall events top up these important reserves of freshwater?).

He has collected over 10 years of data from loggers in Harrie Wood Cave in the Snowy Mountains which count the total number of drips in 15 minute intervals (published recently by an Honours student - Chapman *et al.* 2024 [*Journal of Hydrology*](https://linkinghub.elsevier.com/retrieve/pii/S0022169424001781)).

There were over 40,000 observations in the original data set, collected from nine loggers (labelled HW_2b, HW_4b etc.). Here is a version of the data set that has removed the rows that lacked data for all of the nine loggers in the same time period (>11000 observations).

```{r}
Caves <- read.csv(file = "data/Caves.csv", header = TRUE)
```

To understand the data set, plot the patterns in drip numbers through time for just one of the loggers (the from columns HW-2b, HW_4b etc.). So that R handles years and dates nicely, first format the date column as a date, not character.

```{r}
Caves <- mutate(Caves, Date = as.Date(Date, format = "%d/%m/%Y"))
```

```{r}
ggplot(Caves, aes(Date, HW_4b)) + geom_point(alpha=0.05)
```

The `alpha` argument makes the points transparent - useful when there are very many points plotted on top of each other.

## 12a) Using PCA to visualise variation among years 

Modify your previous code to create a PCA plot that visualises the variation across years with data from all nine loggers. Remember that you'll need to subset the columns to only include the drip data (from columns HW-2b, HW_4b etc.) and bring back the year column into a data frame that has the x and y coordinate values from the ordination plot.

```{r solution 1}
Caves_vars <- Caves %>%
  select(contains("HW"))

caves.pca <- princomp(Caves_vars, cor = FALSE)

summary(caves.pca)

loadings(caves.pca)
```

```{r solution 2}
Caves_xy <- data.frame(caves.pca$scores)

Caves_xy$Year <- as.character(Caves$Year)

ggplot(Caves_xy, aes(Comp.1, Comp.2, colour = Year)) + geom_point()
```
