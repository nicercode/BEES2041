---
title: "Week 7-2 Multivariate methods"
format: 
  html:    
    self-contained: true
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, 
  results = 'hide',
  fig.show= 'hide'
)

# Set the working directory
rprojroot::has_file("BEES2041-code.Rproj") |>
  rprojroot::find_root() |>
  file.path("week 7/Wk7-2-moodle") |>
  setwd()

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)
library(vegan)
```

## Introduction

In this practical, we will continue our look at methods to analyses and visualise complex, **multivariate data sets** - ones where **more than one response variable has been measured from each replicate sample**.

First, we will learn how to run a **hypothesis test** on multivariate data (permutational multivariate analysis of variance or **PERMANOVA**). This will be used to test for differences in species composition in and out of rock pools from the class data set collected at Maroubra.

Second, we will get an introduction to p**rincipal components analysis (PCA)**, a widely used technique to visualise multivariate data.

![](images/nature14140-sf2.jpg){width=70%} <br>


## Key learning objectives

At the end of today's practicals, you should be able to:

-   Understand the logic behind **permutational MANOVA**
-   Understand the purpose of a **principal components analysis (PCA)**
-   Interpret an ordination from a **principal components analysis (PCA)**
-   Explore the output from PCA to **understand relationships among variables**

Let‚Äôs dive in! üöÄ

## Setting up: Packages

For some exercises below, we will be using the datasets from last practical. To avoid repeating yourself and duplicating your code, **add today's code to the notebook you created for the first multivariate practical** rather than creating a new Quarto document. This way, you can then easily re-run previous code to import data and create the objects that you will need for today's work.

We'll again be using the package, called `vegan` and will introduce a new package called `plotly` that will help us visualise multi-dimensional data!

```{r, eval=FALSE}
# install.packages("tidyverse")
# install.packages("vegan")
# install.packages("ploty")
```

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(vegan)
library(plotly)
```

# 7) Nutrient enrichment experiment (multivariate hypothesis testing)
## 7) Nutrient enrichment experiment (multivariate hypothesis testing)

![](images/53711279_1967294661.png){width=70%} <br>

So far, we have been making plots to visualise differences among samples with multiple variables measured per sample, but have not run any statistical tests to determine whether any differences we found are likely to be due to chance sampling or not.

There is a whole world of multivariate statistical tests, but we will show you just a couple to demonstrate how you can test hypotheses with multivariate data.

In the nutrient enrichment experiment, we had: 

- three treatments (control, low and high nutrients)
- 12 response variables (the abundance of each weed species).

 If we had measured just one variable, we would use a linear model and ANOVA to contrast that variable among treatments. With many variables measured, we use **multivariate analyses of variance based on the similarity matrix**.

Instead of partitioning the variation in the actual values of the variables (i.e., abundance of our each of the weed species) as in a ANOVA, this analysis **partitions the variation in the values of the similarity matrix** among and within treatment groups.

An another way to think of this is that **if there was a strong effect of treatment, then all the replicates within each level of treatment are more likely to be similar to each other than they would be to replicates from another level** (i.e., within group similarities higher than among-group similarities).

**Load data**

Let's load the `NutrientEnrichment.csv`, wrangle and transform it for analysis:  

```{r}
# Load data
Nutrients <- read_csv(file = "data/NutrientEnrichment.csv")

# Select weeds only
Nutrients_vars <- select(Nutrients, contains("Weed"))

# Transform data
Nutrients.sq <- sqrt(Nutrients_vars)
```

**Run a PERMANOVA**

We will use the function `adonis2()` in `vegan`. This function perfoms an ANOVA on the similiarity matrix of your choosing:

```{r, results='hide'}
adonis2(Nutrients_vars ~ Nutrients$Treatment, permutations = 999, method = "bray")
```

Let's talk through this code:

- Similar to the model formulae you have seen in `lm()` , the **response** variable are left of the `~` symbol, and the **predictors** variable are on the right (i.e. `Y ~ X` is aimed to predict Y from X). 
- `method = "bray"` tells vegan to use the Bray-Curtis index to represent the similarity between samples, recommended for species-abundance data.

The output should look pretty familiar to you - very similar to an ANOVA table right?

There are some key differences from a univariate (one predictor variable) ANOVA vs a multivariate one. With multivariate data:

-  the analysis is ran on similarities among samples, not the actual values of the original variables

- the probability of the test statistic produced (`pseudo-F`) cannot be obtained from the traditional probability distribution of F. The probabilities here are obtained by **permutation**. From `permutations = 999` of the original data vs the treatment labels, it calculates how often a value of `pseudo-F` as large as yours would occur.


## 7a) Is there evidence to reject the null hypothesis?

`r cloze("yes, the P value is less than 0.05", c("yes, the P value is less than 0.05", "yes, the P-value is greater than 0.05", "no, the P-value is 0.001", "yes, the F value is greater than 1","no, the F value is greater than 9"))`


## Visualise PERMANOVA

We can save the output from `adonis2` and use a density plot to see just how likely our value of pseudo-F was.

```{r, results='hide'}
Nutrients_test <- adonis2(Nutrients_vars ~ Nutrients$Treatment, permutations = 999, method = "bray")

permustats(Nutrients_test) |> densityplot()
```

This produces a frequency histogram of `pseudo-F` obtained from the 999 permutations of the raw data. **This is best viewed as what would happen if the null hypothesis was true** (no similarities between our variables). The `pseudo-F` value you obtained is noted with the vertical black line.

You should have output that shows that this null hypothesis is rejected. This means that: 

 > there is some difference in weed species composition among all three treatments. 
 
**Multivariate contrasts**

 Our next step is to try and understand **which treatment differed from which** e.g contrasts. There is not function to do this, a workaround is to run separate pairwise tests using subsets of data with only two levels of `Treatment`. 

We will use `filter()` to subset rows, and `select()` for subsetting columns for our pairwise comparisons.  The following code creates subsets of rows for each comparison (note that `!=` means ‚Äúnot equals to‚Äù) and runs the **permutational MANOVA** again with just the rows with the weed variables.

**Subset analysis for Control and Low**

```{r, results='hide'}
# Filter treatment is not "High"
Nutrients_ControlvsLow <- filter(Nutrients, Treatment != "High")

# Select columns containing the word "weed"
Nutrients_ControlvsLow_vars <- select(Nutrients_ControlvsLow, contains("Weed"))

# Perform a PERMANOVA using subset data
adonis2(Nutrients_ControlvsLow_vars ~ Nutrients_ControlvsLow$Treatment, permutations = 999, method = "bray")
```

**Subset analysis for Control and High**

```{r, results='hide'}
# Filter treatment is not "Low"
Nutrients_ControlvsHigh <- filter(Nutrients, Treatment != "Low")

# Select columns containing the word "weed"
Nutrients_ControlvsHigh_vars <- select(Nutrients_ControlvsHigh, contains("Weed"))

# Perform a PERMANOVA using subset data
adonis2(Nutrients_ControlvsHigh_vars ~ Nutrients_ControlvsHigh$Treatment, permutations = 999, method = "bray")
```

**Subset analysis for High and Low**

```{r, results='hide'}
# Filter treatment is not "Control"
Nutrients_LowvsHigh <- filter(Nutrients, Treatment != "Control")

# Select columns containing the word "weed"
Nutrients_LowvsHigh_vars <- select(Nutrients_LowvsHigh, contains("Weed"))

# Perform a PERMANOVA using subset data
adonis2(Nutrients_LowvsHigh_vars ~ Nutrients_LowvsHigh$Treatment, permutations = 999, method = "bray")
```

The problem with this approach is increasing the the risk of **false positives**. False positives are also called **type 1 errors**. The idea is, with increasing numbers of pairwise tests being run, by chance, we will detect a statistically significant treatment difference even though the underlying populations may not be different. 

![](images/type_1_errors.png){width=60%} <br>

The simplest correction for this is called the **Bonferonni correction**, where the significance level (Œ± =0.05) is **adjusted by the number of hypotheses being tested**. 

In this example, we will **multiply each P value by three (we have three different treatments)** before interpreting which comparisons are significant (in other words, Œ± for each test becomes `0.05/3`).

## 7b) What do the multivariate tests and their associated probabilities tell you about the differences in species composition among nutrient treatments? Use the pair-wise tests to establish which treatments differ from which. {type=essay}


# 8) Species composition in rock pools (multivariate hypothesis testing)## 8) Species composition in rock pools (multivariate hypothesis testing)

![](images/rock_pool.PNG){width=70%} <br>

For your practical report, you want to accompany the text and figures that contrast species composition in and out of rockpools, with a formal statistical test. You can use a **permutational multivariate analysis of variance** for this, modifying code that you just used for the nutrient enrichment experiment.


# 9) Advanced statistics content!
## 9) Advanced statistics content!

There is considerable current debate in the statistical literature about the best ways to analyse multivariate data sets like the one you just analysed. Analyses that partition variation in the similarity metrics have been criticised for:

-  not accounting for the inevitable problems with homogeneity of variance that are present in environmental data sets. This means that highly abundant species have a very large influence.

-  lacking an underlying model (i.e. you are not estimating any parameters from your data). This means that there aren't tests for interactions in a traditional sense, and normal diagnostics (e.g residual vs fitted plots) are not available. This is a problem because you can't check whether any transformations have actually improved your data.

Research at UNSW by [Professor David Warton's group](http://www.eco-stats.unsw.edu.au/) is focussed on developing methods to better analyse these sorts of data, and they have released an R package 'mvabund' that uses model-based approaches to analyse multivariate data sets. David has even promoted his package with a [video](https://youtu.be/KnPkH6d89l4) drawing on the considerable talents of 80's pop star Rick Astley!

![](images/rick-astley.jpg){width="150"} 
![](images/mvabund_hex.png){width="150"} <br>

# 10) Principal components analysis
## 10) Principal components analysis (PCA)

**PCA** is a commonly used method to **visualise data with many variables**. In this example, we will use it to analyse differences in the shape of leaves.

A plant physiologist is attempting to quantify **differences in leaf shape between two species of tree**. She recorded the following five measurements from ten leaves of each species:

![](images/leaf.png) <br>

**Load data**

Read in the data from `Leafshape.csv`:

```{r}
Leaf_shape <- read_csv(file = "data/Leafshape.csv")

Leaf_shape
```

**Visualise relationships in raw data**

First, let's have a look at the relationships among all five variables. Use the `select()` functions to choose just the columns containing length data then use the `pairs()` function to create a scatterplot matrix

```{r, results='hide'}
# Select only the length data
Leaf_shape_vars <- select(Leaf_shape, -Species)

# Create a scatterplot matrix
pairs(Leaf_shape_vars) 
```

These **relationships among variables** can also be summarised numerically, by calculating the **similarity matrix using the Pearson correlation coefficients**.

**Calculate correlations between variables**

Check the correlations between the all five leaf measurements.

```{r, results='hide'}
cor(Leaf_shape_vars)
```

We obviously can‚Äôt plot these data in five dimensions, but if you look at just three of the five variables, you can get a sense of what PCA is trying to do - **fit a new X-Y plane as close as possible to all points in 5 dimensional space**.

```{r}
# Map variables to axes and set colours
leaf_fig <- plot_ly(Leaf_shape, x = ~Width_2, y = ~Leaf_length, z = ~Width_1, color = ~Species, colors = c('#BF382A', '#0C4B8E'))

# Create interactive plot and add axes labels
fig |>  
  add_markers() |> 
  layout(scene = list(xaxis = list(title = 'Width_2'),
                     yaxis = list(title = 'Leaf_length'),
                     zaxis = list(title = 'Width_1')))

fig
```

## 10a) What makes this data set particularly suitable for PCA?

`r cloze("there are multiple variables that are correlated with each other", c("there are multiple variables that are correlated with each other","there are lots of replicate samples","there are multiple variables measured","there two groups of samples that we want to contrast"))`

## Run a PCA

To conduct a principal components analysis on the leaf data, we can use the `princomp()` function in base stats.

```{r}
leaf.pca <- princomp(Leaf_shape_vars, cor = FALSE)
```

Let's talk through the code: 

- `cor. = FALSE` indicates you want to use the covariance matrix, which is appropriate when all variables are measured on the same scale. Change to `cor = TRUE` to scale the variables and use a correlation matrix (recommended when all the variables are on different scales).

Principal components analysis produces a lot of graphical and numerical output, which can explore by further examining the object (`leaf.pca`) we just created.

## Producing a score plot

This is the ordination of all 20 leaf samples in the new **two-dimensional space defined by principal component 1 and 2**.

```{r, results='hide'}
plot(leaf.pca$scores, pch = 16)
```

You can also produce a **biplot** with the **relationships between the original variables and the principal components overlaid on the score plot**.

```{r, results='hide'}
biplot(leaf.pca)
```

## Understanding how much variance is explained by each component

PCA is aiming to **represent as much variance as possible in the first few principal components**. We can check whether it is doing a good job by exploring the `summary()` of the `leaf.pca` object.

```{r, results='hide'}
summary(leaf.pca)
```

## 10b) Enter the proportion explained by the first two principal components

- PC1: `r cloze_numerical(0.83, tolerance = 0.01)` 
- PC2: `r cloze_numerical(0.22, tolerance = 0.01)`

**Visualise variance explained by princpal components**

These are also visualised by a **scree plot**. Note that the Y-axis presented is not the % of variation explained.

```{r, results='hide'}
screeplot(leaf.pca, type = 'lines')
```

## Understanding how the original variables relate to the principal components

We gain further insight into the **relationships among variables** and help with interpreting our plot by examining the **strength of the correlations between the original variables and the principal components**.

These relationships can be seen in the **biplot** above but also obtained numerically by extracting the **loadings** from the PCA. Note that this output does not print very small values, so the empty cells can be considered close to zero (i.e., no correlation).

```{r, results='hide'}
loadings(leaf.pca)
```

## 10c) Which of the original variables is most strongly correlated with principal component 1?

`r cloze("Width 1", c("Width 1", "Width 2","Length 1","Length 2","Petiole length"))`

## Producing a more effective plot to visualise the data

Let's produce a plot that addresses the question first posed by the researcher. Recall that she wants to quantify **differences in leaf shape between two species of tree**. We can do this by colour coding the samples on the score plot.

Extract the x and y coordinates from the pca object into a data frame.

```{r}
# Extract scores from PCA
Leaf_xy <- data.frame(leaf.pca$scores)
```

Add the labels for the two species to this data

```{r}
# Extract species names from PCA and add to Leaf_xy
Leaf_xy$Species <- Leaf_shape$Species
```

Create a scatter plot with points color coded by species.

```{r, results='hide'}
ggplot(Leaf_xy, aes(Comp.1, Comp.2, color = Species)) + 
  geom_point() + 
  theme_classic()
```

## 10d) Does the principal components analysis suggest any size or shape differences between the leaves of the two species? {type=essay}

## 10e) Using the coefficients and the loadings, which variables are predominantly associated with the separation of species A and B.

`r cloze("Width 1 and Width 2", c("Width 1 and Width 2", "Width 1 and Total length", "Total length and Petiole length", "Only Width 1"))`

# 11) Heavy metals in sediments (PCA)
## 11) Heavy metals in sediments (PCA)

![](images/harbour.jpg){width=70%} <br>

Using the heavy metal data, we can compare the output from cluster analysis, principal components analysis and multidimensional scaling.

```{r}
# Load data
HeavyMetals <- read_csv(file = "data/HeavyMetals.csv")  

# Remove the Site variable
HeavyMetals_vars <- select(HeavyMetals, -Site)
```

**Exercise:**

Conduct a PCA on the heavy metal data. Given that the variables were measured on very different scales, either use a standardised data set and `cor= FALSE` in the `princomp` function or use `cor = TRUE` to run the PCA with a correlation (not covariance) matrix.

```{r solution}
# Perform PCA
metals.pca <- princomp(HeavyMetals_vars, cor = TRUE)

# Extract loadings from PCA
loadings(metals.pca)
```

## 11a) Which variable (i.e., metal) is most strongly correlated with the values of PC1?

`r cloze("Cr", c("Cr","Cu","Mn","Co","Ni","Zn","Cd","Pb","Cr"))`

## 11b) Comparing all your classification or ordination plots, discuss which technique best visualises the patterns in metal contamination among sites. Do any clearly identify sites close the sewage dump? {type=essay}


## 11c) Make an interactive plot of the dataset. Try visualise Ni, Cr, Pb on the x, y, z axis. How does this interactive plot compare with your ordination from last prac? 

```{r, solution2}
# Standardise variables
hm_std <- vegan::decostand(HeavyMetals_vars, "max")

# Grab Site labels
hm_std$Site <- HeavyMetals$Site

# Create mapping of variables
hm_fig <- plot_ly(hm_std, x = ~Ni, y = ~Cr, z = ~Pb, 
                    color = ~Site)

# Generate interactive plot
 hm_fig |>  
  add_markers()
```

# 12) Cave hydrology

![](images/caves.jpg){width=70%} <br>

Professor Andy Baker (School of BEES, UNSW) works in caves to better understand the history of Australia's climate and how climate affects valuable groundwater resources. He looks at the geochemistry of stalagmites to reconstruct climate histories and impacts of fire, and flows of water underground to understand recharge of groundwater (i.e., how do rainfall events top up these important reserves of freshwater?).

He has collected over 10 years of data from loggers in Harrie Wood Cave in the Snowy Mountains which count the total number of drips in 15 minute intervals (published recently by an Honours student - Chapman *et al.* 2024 [*Journal of Hydrology*](https://linkinghub.elsevier.com/retrieve/pii/S0022169424001781)).

There were over 40,000 observations in the original data set, collected from nine loggers (labelled HW_2b, HW_4b etc.). Here is a version of the dataset that has removed the rows that lacked data for all of the nine loggers in the same time period (>11000 observations).
**Load data**
```{r}
Caves <- read_csv(file = "data/Caves.csv")
```

To understand the data set, plot the patterns in drip numbers through time for just one of the loggers (the from columns HW-2b, HW_4b etc.). Let's format the date column as a date, not character so that R handles years and dates nicely. 
**Wrangle data**
```{r}
Caves <- mutate(Caves, Date = as.Date(Date, format = "%d/%m/%Y"))
```

**Visualise data**

Let's visualise the plot the patterns in drip numbers through time for just one of the loggers (`HW_4b`)

```{r, results='hide'}
ggplot(Caves, aes(Date, HW_4b)) + 
  geom_point(alpha=0.05)
```


## 12a) Using PCA to visualise variation among years 

**Exercise:**

- Create a PCA plot that visualises the variation across years with data from all nine loggers. Remember that you'll need to subset the columns to only include the drip data (from columns HW-2b, HW_4b etc.) 
- Bring back the year column into a data frame that has the x and y coordinate values from the ordination plot.

```{r solution1}
# Select data variables only
Caves_vars <- Caves |>
  select(contains("HW"))

# Perform PCA
caves.pca <- princomp(Caves_vars, cor = FALSE)

# ook at output
summary(caves.pca)

# Extract loadings
loadings(caves.pca)

# Extract scores for ordination
Caves_xy <- data.frame(caves.pca$scores)

# Create a year column so we can label them
Caves_xy$Year <- as.character(Caves$Year)

# Create ordination plot
ggplot(Caves_xy, aes(Comp.1, Comp.2, colour = Year)) + 
  geom_point() + 
  theme_minimal()
```

