---
title: "Coral cover prediction"
format: 
  html:
    embed-resources: true
editor: visual
---

Setup

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
# install.packages("ranger")
library(ranger)
```

Load data

```{r}
coral_labelled_raw <- 
  read_csv("data/coral_data_predictors.csv", show_col_types = FALSE)

# Drop some variables
coral_labelled <- 
  coral_labelled_raw |>
  mutate(
    class_num = as.character(class_num),
    dom_num = as.character(dom_num)
    ) |>
  select(-`system:index`, -`.geo`) |>
  select(
    # remove all outputs except coral
    -any_of(c("Algae", "Branching", "Inverts", "Massive", "Plate", "Rock", "Rubble", "Sand", "SoftCoral")), 
    # other variables -- Mitch check, should we keep these
    -any_of(c("random"))#, "pa_branch", "pa_massive", "pa_plate", "pa_soft"))
    #c("class_num", "dom_num", "pa_branch", "pa_massive", "pa_plate", "pa_soft",
    )

coral_labelled |> write_csv("data/coral_labelled.csv")
```

Make training and testing sets

```{r}
npoints <- nrow(coral_labelled)
train_ids <- sample(npoints, 2/3 * npoints)

coral_labelled_train <- coral_labelled |> slice(train_ids)
coral_labelled_test <-  coral_labelled |> slice(-train_ids)
```

Write a function to evaluate within sample and out of sample predictions. For now, simply look at R2 between observed and predicted

```{r}
evaluate <- function(fit, data_train, data_test, ...) {

  # customise prediction function for different methods
  if("ranger" %in% class(fit))
    pred <-  function(fit, data) predict(fit, data=data)$prediction
  else if("glm" %in% class(fit))
    pred <-  function(fit, data) predict(fit, newdata=data)
  else if("lm" %in% class(fit))
    pred <-  function(fit, data) predict(fit, newdata=data)
  
  # add within sample predictions
  data_train <- data_train |> mutate(group = "training",
    predicted = pred(fit,data_train))
  
  # add out of sample predictions
  data_test <- data_test |>  mutate(group = "testing",
    predicted =  pred(fit, data_test))
  
  data_pred <-  rbind(data_train, data_test)  |>
    mutate(
      group = factor(group, levels = c("training", "testing"))
    )

  model_stats <- 
    data_pred |> 
    group_by(group) |> 
    summarise(
      r2 = cor(Coral, predicted)^2,
      RMSE = sqrt(sum((Coral - predicted)^2)/n())
      ) |> 
    ungroup() |>
    mutate(
      x = 0, y=100,
      stats = paste0("RMSE = ", format(RMSE, digits=2), ", R2 =", format(r2, digits=2))
    )
  
  # plot
  data_pred |>
    ggplot(aes(Coral, predicted)) +
    geom_point() +
    geom_smooth(method="lm", formula = 'y ~ x') +
    geom_text(data = model_stats, aes(x, y, label = stats), hjust=0, col="red") +
    labs(x="Observed", y="Predicted") +
    ylim(c(0,100)) + xlim(c(0, 100)) +
    facet_wrap(~group) +theme_classic()
}  
```

# Regression with single predictor

## A single predictor

Let's choose one variable -- e.g. depth_maxent_p

```{r}
fit_lm1 <- lm(Coral ~ depth_maxent_s, data = coral_labelled_train)

evaluate(fit_lm1,  coral_labelled_train, coral_labelled_test)
```

How well does this model do? Terribly!!!

RSE of 21 is bad --\> On average cover has error of 21%

```{r}
library(GGally)
ggpairs(coral_labelled)
```

What about two variables?

```{r}
fit_lm2 <- lm(Coral ~ b1_p + b2_p, data = coral_labelled_train)

evaluate(fit_lm2,  coral_labelled_train, coral_labelled_test)
```

What about with random forest?

```{r}

fit_rf1 <- ranger(Coral ~ b1_p, data = coral_labelled_train)
evaluate(fit_rf1, coral_labelled_train, coral_labelled_test)
```

Model is doing good job of fitting to training data, but high RMSE in testing suggest limited predictive value

### RF with a few covariates

For comparison, here's an RF only 3 covariates.

```{r}
fit_rf3 <- 
  ranger(Coral ~ ., 
         data = coral_labelled_train |> select(Coral, b1_p, b2_p, b3_p)
         )

evaluate(fit_rf2, coral_labelled_train, coral_labelled_test)
```

## All covariates?

Try all!

Standard RF using `ranger` with all covariates

```{r}
fit_rf <- ranger(Coral ~ ., data = coral_labelled_train)

evaluate(fit_rf, coral_labelled_train, coral_labelled_test)
```

This is good, still a drop between training and testing, but now the model is making predictions where error in cover is 13%.

## Is RF better than regression using lm with all covars

```{r}
fit_lm <- lm(Coral ~ ., data = coral_labelled_train)

evaluate(fit_lm,  coral_labelled_train, coral_labelled_test)
```

## How to find best covariates?

What are most important variables?

```{r}
fit_rf <- ranger(Coral ~ ., data = coral_labelled_train, importance = "permutation")

importance(fit_rf) |> sort(decreasing = T)
```

Bigger number means more important for model.

What are most important models?

## How well could we do with just top 5 covars

```{r}
 top_vars <- importance(fit_rf) |> sort(decreasing = T) |> head(10) |> names() 

fit_rf_top5 <- 
  ranger(Coral ~ ., 
         data = coral_labelled_train |> select(Coral, any_of(top_vars))
         )

evaluate(fit_rf_top5, coral_labelled_train, coral_labelled_test)

```

## How well if we don't know if corals present, i.e. without class, dom_num, p\_....

```{r}

fit_rf3 <- 
  ranger(Coral ~ ., 
         data = coral_labelled_train |> select(-dom_num,-class_num)
         )

evaluate(fit_rf3, coral_labelled_train, coral_labelled_test)

```