---
title: "Week 4-2 Generalised linear models"
format: 
  html:    
    self-contained: true
# output:    
#   moodlequiz::moodlequiz:
#     replicates: 1
# moodlequiz:
#   category: "Week 4-2 GLM"
# editor_options: 
#   chunk_output_type: console

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)

# Libraries for prac
library(tidyverse)
library(easystats)

# Data files for prac from BEES2041 root
# data_crabs <- read_csv("week 4/Wk4-2-moodle/data/Crabs.csv")
# data_revegetation <- read_csv("week 4/Wk4-2-moodle/data/Revegetation.csv")
# data_mites <- read_csv("week 4/Wk4-2-moodle/data/mites.csv")
# data_faramea <- read_csv("week 4/Wk4-2-moodle/data/faramea.csv")
```

# Intro to Generalised Linear Models

## Introduction

In this session, weâ€™ll explore **G**eneralized **L**inear **M**odels (**GLMs**), a powerful extension of linear regression that helps us analyse data where relationships arenâ€™t strictly linear. **GLMs** allow us to model different types of responses, making them a go-to tool for many real-world problems.

Weâ€™ll focus on two key types of data:

- **Binomial** data â€“ useful for modeling **probabilities**, like whether an event happens or not (e.g., predicting invasive species presence/absence, or organism surival).
- **Count** data â€“ great for modeling things that happen a certain number of times, like the number of visits to a nesting site or the number of species in a sample.

Weâ€™ll learn how to choose the right model, check if our models fit well, interpret results, . By the end, youâ€™ll be equipped to apply GLMs to your own data!

![Artwork by @allison_horst](images/error-comic.png){width=70%}
<br>

## Key learning objectives

Our learning objective today are:

- **understand** different probaility distribution
- **understand** the concept of a link function
- **run** a generalised linear regression (GLM) in R using `glm()`
- **interpret** the output of a GLM using:
- **extract** the mean and confidence interval for the slope and intercept of the regression line
- **plot** the data and the regression line with confidence intervals

Letâ€™s dive in! ðŸš€ 

## Setting up: Materials

Everything you need for this prac is on Moodle

1. Download this week's materials zip file from Moodle, from the course page
2. Unzip the file by: 
  - MacOS: Double clicking the zipfile 
  - Windows: Right click on the zip file and click "Extract All" 
3. Move the extracted folder into the folder where you store materials for `BEES2041/` 
4. **Click on the Rstudio project file, eg. `Wk-4-2-glms.Rproj`** to open the RStudio project and you're in!!!

We will be working with various datasets collected. These are in the folder `data/`.

You will work in the relevant Quarto document for this prac. Within each Quarto docs there are several challenges for you to complete on your own devices in order to **answer the questions on Moodle**.

## Setting up: Packages

> **Note** that when running R in the browser we need to install the packages each time we start a new session or after you've checked your answers. We're only going to install the parts of `tidyverse` and `easystats` we need for this prac.

```{r, eval=FALSE}
install.packages("dplyr")
install.packages("readr")
install.packages("ggplot2")
install.packages("performance")
install.packages("modelbased")
install.packages("see")
install.packages("emmeans")
install.packages("patchwork")

```

> Remember to load the packages into R to use em!

```{r, results='hide', warning=FALSE, message=FALSE}
library(dplyr)
library(readr)
library(ggplot2)
library(performance)
library(modelbased)
library(patchwork)
```

# GLM Theory

## Why do we need GLMs? 

The standard linear models we used previosuly make several rather srtict assumptions, including: 

- residuals are normally distributed
- constant variance, and 
- a linear repsonses. 

We learned to transform data and  which can help us fit a better linear model but in many cases, this is not enough to fulfill the assumptions of a linear model. 

In these situations,  a **G**eneralized **L**inear **M**odel (**GLM**) is often a good toool. 

**Probablity distributions**

GLMs are a generalisation of linear models that allow us to model different types of responses. As data scientists, we need to choose an appropiate distribution that represents our data. For example: 

- if the response variable is **binary** (e.g., presence/absence), we might use a **binomial** distribution.

![](images/extinct-binary.png){width=40%}

- If the response variable is a **count** (e.g., number of species sighted in an hour), we might use a **Poisson** distribution.

![](images/counts-discrete.png){width=40%}

**Link functions**

A key concept in GLMs is the **link function**.

A link function provides the relationship between the linear model and the mean of the distribution function.  We often denote the outcome of the linear model as $\eta$, where $\eta$ is our usual linear model given by
$$\eta = B0 + B1 * x.$$

As usual, $B0$ is the intercept and $B1$ is the slope of the model. (This is the same as the standard linear model you already know.)

GLMs assume that $\eta$ is related to the mean of the response variable by a link function, $g$, such that 
$$g(\mu_y) = \eta$$
where $\mu_y$ is the mean of the response distribution. 


# GLMs for binary data

## Binomial distribution

The binomial distribution is used to represent **the number of successes in a fixed number of trials**. The binomial distribution has two parameters: 

- the number of trials $n$
- the probability of success $p$. 

The **binomial distribution** generates a series of 0s and 1s, where 1 represents a success.

For example, we can draw 10 numbers (**trials**) from the **binomal distribution** with a **probability of success** of 0.9. You can change the `prob` value and see how that affects **outcome** of the trials

```{r}
rbinom(n = 10, size = 1, prob = 0.9)
```

## Link function for a binomial response

For example, with a **binomial** response, the link function $g$ is the **logit** function:

$$g(\mu_y) = \log\left(\frac{\mu_y}{1-\mu_y}\right)$$

Setting $g(\mu_y) = \eta$ and rearranging gives

$$\mu_y = \frac{e^{\eta}}{1+e^{\eta}}$$

So our mean repsonse is obtained via a transformation of $x$ , creating a model than has a non-linear response, but is still based ona  linear model in $x$. 

It may help to visualise this. Let's plot 

- the linear predictor $\eta$
- the mean of the response variable $\mu_y$

```{r}
#| code-fold: true
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- 2 # slope

data <- tibble(
  x = seq(-5, 5, 0.1)
  ) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)) # mean of the response variable
  )

# Plot the relationship
p1 <- data |>
  ggplot(aes(x = x, y = eta)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(eta),
    title = "Linear model"
  ) +
  theme_minimal()

p2 <- data |>
  ggplot(aes(x = x, y = mu)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal() 

p1 + p2
```

## Response distribution

Now imagine drawing random numbers, where the* *probability of success** is given by the mean of the response variable, $\mu_y$. This is the process we assume has generated the data we observe in the field.

The plot below shows data which were drawn from such a model The black dashed line shows the true relationship between the linear predictor $x$ and the mean of the response variable. The dots show the obseravtions. 

```{r, echo=FALSE}
# Simulate data
set.seed(123) # set seed for reproducibility

data_y <- tibble(
          x = rnorm(40, 1, 2)) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)),  # mean of the response variable
    y = rbinom(n = n(), size = 1, prob = mu) # simulate data
  )

# Plot the data
p3 <-
  data_y |>
  ggplot(aes(x = x, y = y)) +
  geom_point(col = "red") +
  labs(
    x = "x",
    y = "Probability",
    title = "Simulated data"
  ) +
  theme_minimal() +
  # add data from the link function
  geom_line(data = data, aes(x = x, y = mu), color = "black", linetype = "dashed")

p3
```


In the real world, we don't know the true relationship between $x$  and $y$; we have to estimate this from the data. This is where GLMs come in!

So let's imagine we have the data above, but don't know the true relationship. 

We can fit a GLM to the data to estimate the relationship:

```{r}
# Fit the model
model <- glm(y ~ x, data = data_y, family = "binomial") 

# Estimate the means
means <- estimate_means(model, by = "x", length = 100)
```

Now we can plot the data and the fitted model:

```{r, echo=FALSE}
# add to plot above
p3 +
  geom_line(data = means, aes(x = x, y = Probability), color = "red") +
  geom_ribbon(data = means, aes(x = x, y = Probability, ymin = CI_low, ymax = CI_high), fill = "grey", alpha = 0.2)
```


We can extract the slope of the fitted model above using the `parameters()` function:
```{r}
parameters(model)
```

Note the true values we used to simulate the data are $B_0 = 0$ and $B_1 = 2$. So the model has done a reasonable job of estimating the slope, and the true values are included in the 95% confidence interval.

## Worked Example: Crabs along the beach

Let's apply the theory we learned about with some data collected by some students our department who had surveyed a beach. This group of students wanted to analyse **how the presence of a crab varied with time and distance from the water line**. 

The response variable is **discrete**: 

- `CrabPres` the **presence or absence** of a crab in a given sighting 

The predictor is **continuous**:

- `Dist` 

## Read in data

Let's load the data and have a look at the first few rows of the dataset

```{r}
data_crabs <- read_csv("data/Crabs.csv")

data_crabs
```

## Look at raw data

Its always good practice to take a look at your raw data, especially your response and explanatory/predictor variable.

```{r}
ggplot(data_crabs, aes(x = Dist, y = CrabPres)) + 
  geom_jitter(height = 0, width = 0.2, alpha = 0.2, size = 4, colour = "red") + 
  theme_minimal()
```

## Fit the model

Let's fit a **GLM** to **binomial** data e.g. presence/absence of crabs (0/1). GLMs uses very similar syntax to fitting linear models. 

We use the `glm()` function instead of `lm()`. We need to add a `family` argument to specify the **distribution** you want to represent your response variable. We will use `family = "binomial"`.

```{r}
fit_crab <- glm(CrabPres ~ Dist, family = binomial, data = data_crabs)
```

## Look at model output

Let's take a look at the output `paramaters()`.

We interpret this output in a similar manner as for linear model

```{r}
parameters(fit_crab)
```

There is no evidence that the presence of crabs varies with distance along the shoreline (p = 0.364). 

## Visualising our model estimates

We use `estimate_means()` to do the maths for us and calculate the mean probability of sighting a crab by the different levels of distance we had in our data.

Then we can visualise this with a graphic.

```{r}
crab_means <- estimate_means(fit_crab, by = "Dist")

ggplot(crab_means, aes(x = Dist, y = Probability)) +
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high),width = 0.2) + 
  geom_point(size = 5, col = "darkorange") + 
  theme_minimal()
```

## Example 1: Mites 

![](images/mites.jpg){width=60%}

**Exercises:**

- Fit an approaiate model for the presence/absence of Galumna sp. `pa` as a function of substrate density `SubsDens` as follows, using the `glm()` function and the `family` argument
- You may need to make some plots to help you answer the following questions
- Use your model output to elp you answer the questions

```{r}
# Load in data
mites <- read_csv("data/mites.csv")

# Raw data - scatterplot
ggplot(mites, aes(x = SubsDens, y = pa)) + 
  geom_point() + 
  theme_minimal()

# Fit a model
mite_subs_fit <- glm(pa ~ SubsDens,
                 data = mites,
                 family = "binomial")

# Look at model output
parameters(mite_subs_fit)
```

**Questions:**

1. **What type of variable is `pa`, why do we need a GLM to model this data?**

`pa` is a `r cloze("binary", c("binary", "continuous", "random"))` variable, 1 indicates `r cloze("presence", c("absence", "presence"))` of mites. We need a GLM to model this data, because this data is not `r cloze("continuous", c("binary", "continuous", "random"))` and comes from a `r cloze("non-normal", c("normal", "non-normal"))` distribution and cannot be represented by a linear model. 

2. **What does the Intercept represent in this model?**

The Intercept in this context represents the probability of observing the Galumna mite when substrate density is `r cloze("0", c("1", "0", "100"))`

3. **What sign is the slope? What does this mean for `pa`?**

The slope `SubsDen` is `r cloze("positive", c("positive", "negative"))`. This means the probability of observing the mite `r cloze("increases", c("decreases", "increases"))` with substrate density.

4. **Interpret the P-value for the slope `SubDens`**

The p-value of `r cloze("0.360",c("0.360", "0.123", "0.915"))` for the slope `SubDens`. This indicates that there is `r cloze("moderate", c("moderate", "low", "high"))` probability there is `r cloze("no", c("a strong", "no"))` relationship between the presence of mites and substrate density

## Example 2: Bacterial treatment 

![](images/bacteria.jpeg){width=60%}

Using the bacteria dataset (from the `MASS` R package), try and model the presence of _H. influenzae_ as a function of week of test. 

Medical researchers collected data on the presence of the bacteria H. influenzae in children with _Otitis media_ in the Northern Territory of Australia. 50 children with a history of _Otitis media_ were tested to assess the effects of a drug on the disease. The children were randomised to the drug treatment or a placebo treatment. The presence of _H. influenzae_ was checked at weeks 0, 2, 4, 6 and 11. 

**Exercises:**

- load the bacteria dataset.
- you'll need to convert the `y` variable to a binary format 
- Fit an appropiate model for this type of data
- Use the output to answer the following questions

```{r}
# generate data
# MASS::bacteria |> mutate(
#   y = ifelse(y == "y", 1, 0)) |> write_csv("data/bacteria.csv")

# Look at data
bacteria <- read_csv("data/bacteria.csv") 

# Look at data
head(bacteria)

# Fit a GLM
bacteria_fit <- glm(y ~ week, data = bacteria, family = "binomial")

# Look at model output
parameters(bacteria_fit)

# Estimate means
means <- estimate_means(bacteria_fit, by = "week", length = 100)

# Plot the data
ggplot(bacteria, aes(x = week, y = y)) +
  geom_jitter(width=0.1, height = 0.1) +
  geom_line(data = means, aes(x = week, y = Probability), col = "red") +
  theme_minimal()
```

**Questions:**

1. **What type of variable is `y`? What distribution can we use to represent it well?** _Hint: Make a scatterplot and see for yourself_

`y` is the `r cloze("presence and absence/", c("presence and absence/", "count", "denisty"))` of the bacteria _H. influenzae_. A `r cloze("binomial",c("binomial", "Poisson", "normal"))` distribution will represent this type of data well. 

2. **Interpret the slope `week`**

The `r cloze("negative", c("negative", "positive"))` value for the slope indicates that the probability of detecting the bacteria _H. influenzae_ `r cloze("decreases", c("decreases", "increases"))` as weeks pass.

3. **What is the p-value for the slope `week` represent?**

The p-value for the slope `week` represents the probabilty there is `r cloze("no", c("no", "a strong", "a weak"))` relationship between the `r cloze("presence of bacteria H. influenzae", c("presence of bacteria H. influenzae", "intercept"))` with `r cloze("week", c("week", "patient age", "patient location"))`.

4. **What does the p-value for the slope `week` mean then?**

The p-value is `r cloze("0.011", c("0.011", "0.1", "11"))`. This indicates there is relatively `r cloze("low", c("low", "high"))` chance that there is no relationship between the presence of bacteria _H. influenzae_ and weeks.

# GLMs for count data

## Poisson distribution

The **Poisson distribution** is used to represent a **count of events per unit of sampling** (usually time, space or volume). For example, the number of invasive weeds in a quadrat. The Poisson distribution has one parameter $\lambda$, which is the **mean number of events per unit of sampling**.

The **Poisson distribution** generates a list of integers (whole numbers), starting at 0, representing the number of events that took place.

For example, if we count the number of events in 10 sampling events with mean rate of 1.5 events per unit, the data may look like this. 

```{r}
rpois(n = 10, lambda = 1.5)
```

Change the `lambda` value to see how that changes the number of events observed per sample.

## Link function for a Poisson response

The link function $g$ for the Poisson distribution is the log function, such that

$$g(\mu_y) = \log(\mu_y)$$

Rearranging gives

$$\mu_y = e^{\eta}$$

So the mean of the response variable is given by the exponential of $\eta$. 

Plotting the function shows the relationship between the linear predictor $x$ with both $\eta$ and $\mu_y$.

```{r, echo=FALSE}
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- 0.8 # slope

data <- tibble(
  x = seq(-5, 5, 0.1)
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) # mean of the response variable
  )

# Plot the relationship
p1 <- data |>
  ggplot(aes(x = x, y = eta)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(eta),
    title = "Linear model"
  ) +
  theme_minimal()

p2 <- data |>
  ggplot(aes(x = x, y = mu)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal() 

p1 + p2
```

As you can see from the plot above, the mean of the response variable is a non-linear function of $x$, and the average rate of events increases exponentially with the linear predictor. 

In other words, the higher $x$, the more events are predicted to occur in a given preiod. 

## Response distribution

Now imagine drawing random numbers, where the avarerage rate of events is given by $\mu_y$. In any one period, you could have fewer, or more than $\mu_y$ by chance. But overall, the number of events will follow $\mu_y$. 

```{r, echo = FALSE}
# Simulate data
set.seed(123) # set seed for reproducibility

data_y <- tibble(
          x = rnorm(40, 1, 2)) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta),  # mean of the response variable
    y = rpois(n = n(), lambda = mu) # simulate data
  )

# Plot the data
p3 <-
  data_y |>
  ggplot(aes(x = x, y = y)) +
  geom_point(col="red") +
  labs(
    x = "x",
    y = "y",
    title = "Simulated data"
  ) +
  theme_minimal() +
  # add data from the link function
  geom_line(data = data, aes(x = x, y = mu), color = "black", linetype = "dashed")

p3
```


In the real world, we don't know the true relationship x and the response variable; we have to estimate this from the data.

So let's imagine we have the data above, but don't know the true relationship between $x$  and the mean of the response variable. 

We can fit a GLM to the data to estimate the relationship:

```{r}
# Fit the model
model <- glm(y ~ x, data = data_y, family = "poisson") 

# Estimate the means
means <- estimate_means(model, by = "x", length = 100)
```

Now we can plot the data and the fitted model:

```{r, echo = FALSE}
# add to plot above
p3 +
  geom_line(data = means, aes(x = x, y = Mean), color = "red") +
  geom_ribbon(data = means, aes(x = x, y = Mean, ymin = CI_low, ymax = CI_high), fill = "grey", alpha = 0.2)
```


We can extract the slope of the fitted model above using the `parameters()` function:
```{r}
parameters(model)
```

Note the true values we used to simulate the data are $B_0 = 0$ and $B_1 = 0.8$. So the model has done a reasonable job of estimating the slope, and the true values are included in the 95% Confidnce interval.


## Worked Example: Slugs during revegetation

![Ya snailed it!](images/snails.jpg){width=60%}

Let's work with some data collected by some students in our department who wanted to investigate **how the abundnace of slbus varied under two experimental treatemnts**. 

The response variable is **discrete**: 

- `Soleolifera` is the latin name for the order of terrestrial slugs. This column gives us the counts of this taxa.

The predictor is **discrete**:

- `Treatment` 


## Read in data

Let's load the data and have a look at the first few rows of the dataset

```{r}
data_revegetation <- read_csv("data/Revegetation.csv")

data_revegetation
```

## Look at raw data

```{r}
ggplot(data = data_revegetation, aes(x = Treatment, y = Soleolifera))  + 
  geom_violin(fill = NA) + 
  geom_jitter(aes(colour = Treatment),alpha = 0.5, width = 0.2, size = 2 ) +   theme_minimal() + 
  coord_flip()
```

## Fit the model

```{r}
slug_fit <- glm(Soleolifera ~ Treatment, family = "poisson", data = data_revegetation)
```

## Look at model output

```{r}
parameters(slug_fit)
```

Our model indicates strong evidence of a treatment effect  (p < 0.001). 

## Visualising our model estimates

```{r}
slug_means <- estimate_means(slug_fit, by = "Treatment")

ggplot(slug_means, aes(x = Treatment, y = Mean)) + 
  geom_col(aes(colour = Treatment, fill = Treatment)) + 
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high),width = 0.2) + 
  theme_minimal()
```

There is strong evidence of a positive effect of bush regeneration on the abundance of slugs from the order Soleolifera (p < 0.001)

## Example 3: Faramea trees in Panama

![](images/faramea.jpeg){width=60%}

The dataset contains the number of trees of the species _Faramea occidentalis_  measured in 43 quadrants in Barro Colorado Island in Panama. For each quadrant, environmental characteristics were also recorded such as elevation or precipitation. Let us take a look at the number of Faramea occidentalis found at each quadrant.

**Exercises:**

- Load the dataset into R
- Fit an appropiate model for this type of data
- Use the output to answer the following questions
- You may need to make some plots to help answer the questions

```{r}
# Load data
faramea <- read_csv("data/faramea.csv")

# Make a histogram
ggplot(faramea, aes(x = Faramea.occidentalis)) +
  geom_histogram() +
  theme_minimal()

# Fit a Poisson GLM
faramea_fit <- glm(Faramea.occidentalis ~ Elevation,
  data = faramea, family = poisson
)

# Look at model output
parameters(faramea_fit) |> tibble()

# Estimate means
means <- estimate_means(faramea_fit, by = "Elevation")

# Visualise means
ggplot(means, aes(x = Elevation, y = Mean)) +
  geom_ribbon(aes(ymin = CI_low, ymax = CI_high), fill = "grey") +
  geom_line() +
  geom_point(data = faramea, aes(x = Elevation, y = Faramea.occidentalis), col = "red") +
  theme_classic()

```

**Questions:**

1. **Make a histogram of `Faramea.occidentalis` in the data `faramea`. What type of data is this?**

The histogram shows that the counts of `Faramea.occidentalis` are `r cloze("right-skewed", c("right-skewed", "left-skewed"))`. The variable consists of mostly `r cloze("0s", c("0s", "10s",  "20s"))`. The variable `Faramea.occidentalis` is considered `r cloze("count",c("count","binary", "continuous"))` data and would be best represented by a `r cloze("binomial",c("binomial", "normal", "Poisson"))` distribution. 

2. **Interpret the slope for the predictor `Elevation`**

The slope is `r cloze("negative", c("positive", "negative"))`, indicating the counts for _Faramea occidentalis_ `r cloze("decreases", c("decreases", "increases"))` with increasing `r cloze("elevation",c("density", "elevation", "temperature"))`.

3. **Interpret the CI for `Elevation`.** _Hint: you may need to pipe `parameters(faramea_fit)` into  `tibble()`_

The CI for the slope is  `r cloze("-0.00410", c("1.55", "-0.00410", "0.110"))` (lower) and `r cloze("-0.00157", c("-0.00157", "1.98", "0.95"))` (upper). We are `r cloze("95%", c("5%", "95%", "97.5%"))` sure the true `r cloze("population", c("sample", "population"))` value of the `r cloze("slope", c("slope", "intercept", "counts"))` lies within this range.

## Example 4: Mite counts with water content

![](images/mites.jpg){width=60%}

Using the `mites` data, try model the abundance of the species `Galumna` as a function of water content `WatrCont`.

**Exercises:**

- Load the dataset into R
- Fit an appropiate model for this type of data
- Use the output to answer the following questions
- You may need to make some plots to help answer the questions

```{r}
# Load data
mites <- read_csv("data/mites.csv")

# Raw data - histogram
ggplot(mites, aes(x = Galumna)) + 
  geom_histogram()

# Raw data - scatterplot
ggplot(mites, aes(x = WatrCont, y = Galumna)) + 
  geom_point()

# Fit model
galumna_fit = glm(Galumna~WatrCont, data=mites, family = "poisson")

# Look at model output
parameters(galumna_fit)

# Estimate means
galumna_means <- estimate_means(galumna_fit, by = "WatrCont")

# Visualise means
ggplot(galumna_means, aes(x = WatrCont, y = Mean)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = CI_low, ymax = CI_high, y = Mean), width = 20)
```

**Questions:**

1. **Make a histogram of `Galumna` counts, describe its distribution.**

The distribution for `Galumna` is `r cloze("right-skewed", c("right-skewed", "left-skewed"))`, meaning there are lots of `r cloze("small", c("small", "large"))` values in the variable. This data is count data and resembles a `r cloze("Poisson", c("binomial", "Poisson", "normal"))`.

2. **Make a scatterplot of `Galumna` and `WatrCont.` Why do you think the data is not suited for a linear model?**

The scatterplot shows that the data is not `r cloze("continuous", c("continuous", "Poissrandomon", "corrected"))` (there are horizontal lines of points). The plot also shows that the points are `r cloze("not evenly spread out", c("outliers", "not evenly spread out"))`. These characteristics suggests that a linear model is not suited for this type of data.  

3. **Interpret the slope `WatrCont` and its p-value.**

The slope for WaterCont is `r cloze("negative", c("positive", "negative"))`, suggesting that as water content `r cloze("increases", c("decreases", "increases"))`, the counts of Galumna decreases. The p-value tells us that there is very `r cloze("low", c("low", "high"))` probability that there is no relationship between water content and mite count.

4. **Make a plot displaying the mean counts for Galumna at different `WatrCont`, at what level of WatrCont do we still mite counts start to taper off?**

The `r cloze("mean", c("minium", "mean", "maximum"))`  counts for Galumna, starts to taper off at `r cloze("600", c("200", "400", "600"))`.

# Extensions

## Binomial: Varying the slope and intercept of the model

Assuming you made it this far, you may be interested in how the slope and intercept of the model affect the mean of the response variable.

**Varying the slope of the model**

Like a linear model, the slope of the model determines the relationship between the linear predictor and the mean of the response variable. However, the effects are modified by the link function and no longer linear.

The following plot shows the mean of the response variable for different slopes of the binomial model.

```{r, echo=FALSE}
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- c(0, 0.1, 1, 5) # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B1))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Slope")
```

As you can see,

- Varying the slope of model will make the logistic function steeper or shallower.
- The intercept of the model determines the mean of the response variable when the linear predictor is zero.

**Varying the intercept of the model**

```{r, echo = FALSE}
# Create a sequence of linear predictors
B0 <- c(-2, 0, 2) # intercept
B1 <- 2 # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B0))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Intercept")
```


## Poisson: Varying the slope and intercept of the model


**Varying the slope of the model**

The following plot shows the mean of the response variable for different slopes of the Poisson model.

```{r, echo=FALSE}
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- c(0, 0.2, 0.8) # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B1))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Slope")
```

**Varying the intercept of the model**

Changing the intercept of the model will shift the distribution up and down. Technically, it specifies  the value of log(y) when x = 0.

```{r, echo = FALSE}
# Create a sequence of linear predictors
B0 <- c(-2, 0, 2) # intercept
B1 <- 0.8 # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B0))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Intercept")
```





