---
title: "Week 4-2 Generalised linear models"
format: 
  html:    
    self-contained: true
# output:    
#   moodlequiz::moodlequiz:
#     replicates: 1
# moodlequiz:
#   category: "Week 4-2 GLM"
# editor_options: 
#   chunk_output_type: console

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)

# Libraries for prac
library(tidyverse)
library(easystats)

# Data files for prac from BEES2041 root
# data_crabs <- read_csv("week 4/Wk4-2-moodle/data/Crabs.csv")
# data_revegetation <- read_csv("week 4/Wk4-2-moodle/data/Revegetation.csv")
# data_mites <- read_csv("week 4/Wk4-2-moodle/data/mites.csv")
# data_faramea <- read_csv("week 4/Wk4-2-moodle/data/faramea.csv")
```

# Intro to Generalised Linear Models

## Introduction

In this session, weâ€™ll explore **G**eneralized **L**inear **M**odels (**GLMs**), a powerful extension of linear regression that helps us analyse data where relationships arenâ€™t strictly linear. **GLMs** allow us to model different types of responses, making them a go-to tool for many real-world problems.

Weâ€™ll focus on two key types of data:

- **Binomial** data â€“ useful for modeling **probabilities**, like whether an event happens or not (e.g., predicting invasive species presence/absence, or organism surival).
- **Count** data â€“ great for modeling things that happen a certain number of times, like the number of visits to a nesting site or the number of species in a sample.

Weâ€™ll learn how to choose the right model, check if our models fit well, interpret results, . By the end, youâ€™ll be equipped to apply GLMs to your own data!

![Artwork by @allison_horst](images/error-comic.png){width=70%}
<br>

## Key learning objectives

Our learning objective today are:

- **understand** different probaility distributions
- **understand the concept of a link function**
- **run** a **generalised linear regression** in R using `glm()`
- **run diagnostic checks** on your  model output using `check_model()` to assess whether it meets the assumptions of a linear model.
- **interpret** the output of a **generalised linear regression** using:
  - `parameters()`
- **extract** the **confidence interval** for the **slope** and **intercept** of the **regression** line
  - `estimate_means()`
- **plot** the data and the **regression** line
  - add the **confidence interval** using output of `estimate_means()`

Letâ€™s dive in! ðŸš€ 

## Setting up

**Materials:**

Everything you need for this prac is on Moodle

1. Download this week's materials zip file from Moodle, from the course page
2. Unzip the file by: 
  - MacOS: Double clicking the zipfile 
  - Windows: Right click on the zip file and click "Extract All" 
3. Move the extracted folder into the folder where you store materials for `BEES2041/` 
4. **Click on the Rstudio project file, eg. `Wk-4-2-glms.Rproj`** to open the RStudio project and you're in!!!

We will be working with various datasets collected. These are in the folder `data/`.

You will work in the relevant Quarto document for this prac. Within each Quarto docs there are several challenges for you to complete on your own devices in order to **answer the questions on Moodle**.

**Setting up: Packages:**

We will also be working with packages from the `tidyverse` and `easystats`, building on skills from previous pracs. You should have these already installed on your machines.

> **Note** that when running R in the browser we need to install the packages each time we start a new session or after you've checked your answer

In case you don't have them installed, here is code for you to do so:
```{r, eval=FALSE}
# Uncomment and run only the lines below only if you have not previously installed these.
# install.packages("tidyverse")
# install.packages("easystats")
```

> Remember to load the packages into R to use em! 

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(easystats)
library(patchwork)
```


# GLM Theory

## Why do we need GLMs? 

Linear models assume constant variance. We learned to transform data and  which can help us fit a better linear model but in some cases, it can be hard to fulfill the assumptions of a linear model. 

For example, discrete response data, like counts and presence/absence data, generally exhibit a mean-variance relationship. This means, for counts that are on average 5, we would expect most samples to be between about 1 and 9, but for counts that are on average 500, most of the observations will tend to be between 450 and 550, giving us a much larger variance when the mean is large. 

In situations where our response is discrete and exhibit a mean-variance relationship, we turn another tool - GLMs! 

## Probablity distributions

**Linear models** also assumes the underlying distribution for the response variable is **normal**, but sometimes that assumption is violated, meaning our data comes from a **non-normal distribution**. We used **GLMs** when working with **non-normal distributions**

As data scientists, we need to choose an appropiate distribution that represents our data. For example: 

- if the response variable is **binary** (e.g., presence/absence), we might use a **binomial** distribution.

- If the response variable is a **count** (e.g., number of species sighted in an hour), we might use a **Poisson** distribution.

## Binomial distribution

The binomial distribution is used to represent **the number of successes in a fixed number of trials**. The binomial distribution has two parameters: 

- the number of trials $n$
- the probability of success $p$. 

The **binomial distribution** generates a series of 0s and 1s, where 1 represents a success.

For example, we can draw 10 numbers (**trials**) from the **binomal distribution** with a **probability of success** of 0.9. You can change the `prob` value and see how that affects **outcome** of the trials

```{r}
rbinom(n = 10, size = 1, prob = 0.9)
```

## Poisson distribution

The **Poisson distribution** is used to represent a **count of events per unit of sampling** (usually time, space or volume). For example, the number of invasive weeds in a quadrat. The **Poisson distribution**has one parameter $\lambda$, which is the **mean number of events per unit of sampling**.

The **Poisson distribution** generates a list of integers (whole numbers), starting at 0, representing the number of events that took place.

For example, we can count the number of events in 10 sampling events with mean rate of 1.5 events per unit. Change the `lambda` value to see how that changes the number of events observed.

```{r}
rpois(n = 10, lambda = 1.5)
```

## Link functions

A key concept in GLMs is the **link function**.

A link function provides the relationship between  $x$ and the mean of the distribution function.  We often denote $x$  as $\eta$, where $\eta$ is our usual linear model given by

$$\eta = B0 + B1 * x.$$
As usual, $B0$ is the intercept and $B1$ is the slope of the model. (This is the same as the standard linear model you already know.)

GLMs assume that $\eta$ is related to the mean of the response variable by a link function, $g$, such that 
$$g(\mu_y) = \eta$$
where $\mu_y$ is the mean of the response distribution. 

For example, with a bionomial response, the link function $g$ is the logit function:

$$g(\mu_y) = \log\left(\frac{\mu_y}{1-\mu_y}\right)$$

Setting $g(\mu_y) = \eta$ and rearranging gives

$$\mu_y = \frac{e^{\eta}}{1+e^{\eta}}$$

So our mean repsonse is obtained via a transformation of $x$ , creating a non-linear model. 

# Theory Binomial 

## Link function for a binomial response

It may help to visualise the link functions. Let's consider a binomial response, where the mean of the response variable is given by the logistic function above.  Plotting the function shows the relationship between $x$  and the mean of the response variable.

```{r}
#| code-fold: true
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- 2 # slope

data <- tibble(
  x = seq(-5, 5, 0.1)
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)) # mean of the response variable
  )

# Plot the relationship
p1 <- data |>
  ggplot(aes(x = x, y = eta)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(eta),
    title = "Linear model"
  ) +
  theme_minimal()

p2 <- data |>
  ggplot(aes(x = x, y = mu)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal() 

p1 + p2
```

## Response distribution

Now imagine drawing random numbers, where the probability of success is given by the mean of the response variable, $\mu_y$. This is the process we assume has generated the data we observe in the field.

The plot below shows data which were drawn from such a model The black dashed line shows the true relationship between the linear predictor $x$ and the mean of the response variable. The dots show the obseravtions. 

```{r, echo=FALSE}
# Simulate data
set.seed(123) # set seed for reproducibility

data_y <- tibble(
          x = rnorm(40, 1, 2)) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)),  # mean of the response variable
    y = rbinom(n = n(), size = 1, prob = mu) # simulate data
  )

# Plot the data
p3 <-
  data_y |>
  ggplot(aes(x = x, y = y)) +
  geom_point(col = "red") +
  labs(
    x = "x",
    y = "Number of events",
    title = "Simulated data"
  ) +
  theme_minimal() +
  # add data from the link function
  geom_line(data = data, aes(x = x, y = mu), color = "black", linetype = "dashed")

p3
```


In the real world, we don't know the true relationship between $x$  and the mean of the response variable. We have to estimate this from the data. This is where GLMs come in!

So let's imagine we have the data above, but don't know the true relationship. 


We can fit a GLM to the data to estimate the relationship:

```{r}
# Fit the model
model <- glm(y ~ x, data = data_y, family = "binomial") 

# Estimate the means
means <- estimate_means(model, by = "x", length = 100)
```

Now we can plot the data and the fitted model:

```{r, echo=FALSE}
# add to plot above
p3 +
  geom_line(data = means, aes(x = x, y = Probability), color = "red") +
  geom_ribbon(data = means, aes(x = x, y = Probability, ymin = CI_low, ymax = CI_high), fill = "grey", alpha = 0.2)
```


We can extract the slope of the fitted model above using the `parameters()` function:
```{r}
parameters(model)
```

Note the true values we used to simulate the data are $B_0 = 0$ and $B_1 = 2$. So the model has done a reasonable job of estimating the slope, and the true values are included in the 95% Confidnce interval.


## Varying the slope and intercept of the model

Like a linear model, the slope of the model determines the relationship between the linear predictor and the mean of the response variable.

The intercept of the model determines the mean of the response variable when the linear predictor is zero.

Varying the slope of model will make the logistic function steeper or shallower.

```{r, echo=FALSE}
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- c(0, 0.1, 1, 5) # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B1))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Slope")
```

## Varying the intercept of the model

```{r, echo = FALSE}
# Create a sequence of linear predictors
B0 <- c(-2, 0, 2) # intercept
B1 <- 2 # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) / (1 + exp(eta)) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B0))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Intercept")
```

# Theory Poisson

## Link function for a Poisson response

Now let's consider a Poisson response. Here the link function $g$ is the log function, such that

$$g(\mu_y) = \log(\mu_y)$$

Rearranging gives

$$\mu_y = e^{\eta}$$

So the mean of the response variable is given by the exponential of $\eta$. Plotting the function shows the relationship between the linear predictor and the mean of the response variable.

```{r, echo=FALSE}
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- 0.8 # slope

data <- tibble(
  x = seq(-5, 5, 0.1)
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) # mean of the response variable
  )

# Plot the relationship
p1 <- data |>
  ggplot(aes(x = x, y = eta)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(eta),
    title = "Linear model"
  ) +
  theme_minimal()

p2 <- data |>
  ggplot(aes(x = x, y = mu)) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal() 

p1 + p2
```

So the rate of events increases exponentially with the linear predictor. In other words, the higher $x$, the more events are predicted to occur in a given preiod. 

## Response distribution

Now imagine drawing random numbers, where the avarerage rate of events is given by $\mu_y$. In any one period, you could have fewer, or more than $\mu_y$ by chance. But overall, the number of events will follow $\mu_y$. 

```{r, echo = FALSE}
# Simulate data
set.seed(123) # set seed for reproducibility

data_y <- tibble(
          x = rnorm(40, 1, 2)) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta),  # mean of the response variable
    y = rpois(n = n(), lambda = mu) # simulate data
  )

# Plot the data
p3 <-
  data_y |>
  ggplot(aes(x = x, y = y)) +
  geom_point(col="red") +
  labs(
    x = "x",
    y = "y",
    title = "Simulated data"
  ) +
  theme_minimal() +
  # add data from the link function
  geom_line(data = data, aes(x = x, y = mu), color = "black", linetype = "dashed")

p3
```


In the real world, we don't know the true relationship between the linear predictor and the mean of the response variable. We have to estimate this from the data. This is where GLMs come in!

So let's imagine we have the data above, but don't know the true relationship between $x$  and the mean of the response variable. 

We can fit a GLM to the data to estimate the relationship:

```{r}
# Fit the model
model <- glm(y ~ x, data = data_y, family = "poisson") 

# Estimate the means
means <- estimate_means(model, by = "x", length = 100)
```

Now we can plot the data and the fitted model:

```{r, echo = FALSE}
# add to plot above
p3 +
  geom_line(data = means, aes(x = x, y = Mean), color = "red") +
  geom_ribbon(data = means, aes(x = x, y = Mean, ymin = CI_low, ymax = CI_high), fill = "grey", alpha = 0.2)
```


We can extract the slope of the fitted model above using the `parameters()` function:
```{r}
parameters(model)
```

Note the true values we used to simulate the data are $B_0 = 0$ and $B_1 = 0.8$. So the model has done a reasonable job of estimating the slope, and the true values are included in the 95% Confidnce interval.


## Varying the slope and intercept of the model

Like a linear model, the slope of the model determines the relationship between $x$  and the mean of the response variable.

The intercept of the model determines the mean of the response variable when $x$  is zero.

Varying the slope of model will make the logistic function steeper or shallower.

```{r, echo=FALSE}
# Create a sequence of linear predictors
B0 <- 0 # intercept
B1 <- c(0, 0.2, 0.8) # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B1))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Slope")
```

## Varying the intercept of the model

Changing the intercept of the model will shift the distribution up and down. Technically, it specifies  the value of log(y) when x = 0.

```{r, echo = FALSE}
# Create a sequence of linear predictors
B0 <- c(-2, 0, 2) # intercept
B1 <- 0.8 # slope

data <- expand_grid(
  x = seq(-5, 5, 0.1),
  B0 = B0,
  B1 = B1
) |>
  mutate(
    eta = B0 + B1 * x, # linear predictor
    mu = exp(eta) # mean of the response variable
  )

# Plot the relationship
data |>
  ggplot(aes(x = x, y = mu, color = as.character(B0))) +
  geom_line() +
  labs(
    x = "x",
    y = expression(mu[y]),
    title = "Mean of the response variable"
  ) +
  theme_minimal()  +
  theme_classic() +
  scale_color_discrete(name = "Intercept")
```


# GLM Binomial

## Worked Example: Crabs along the beach

Let's work with some data collected by some students our department who had surveyed a beach. This group of students wanted to analyse **how the presence of a crab varied with time and distance from the water line**. 

The response variable is **discrete**: 

- `CrabPres` the **presence or absence** of a crab in a given sighting 

The predictor is **continuous**:

- `Dist` 

## Read in data

Let's load the data and have a look at the first few rows of the dataset

```{r}
data_crabs <- read_csv("data/Crabs.csv")

data_crabs
```

## Look at raw data

```{r}
ggplot(data_crabs, aes(x = Dist, y = CrabPres)) + 
  geom_jitter(height = 0, width = 0.2, alpha = 0.2, size = 4, colour = "red") + 
  theme_minimal()
```

## Fit the model

Let's fit a **GLM** to **binomial** data e.g. presence/absence of crabs (0/1). =GLMs uses very similar syntax to fitting linear models. 

We use the `glm()` function instead of `lm()`. We need to add a `family` argument to specify the **distribution** you want to represent your response variable. We will use `family = "binomial"`.

```{r}
fit_crab <- glm(CrabPres ~ Dist, family = binomial, data = data_crabs)
```

## Look at model output

Let's take a look at the output `paramaters()`.

These output is interpreted in a similar manner as for linear model, but we need to include the link function, $g$.

```{r}
parameters(fit_crab)
summary(fit_crab)
```

If $p$ is the probability of crab presence, this output tells us:

$logit(p) = -1.02 + 0.06 * Dist$ 

There is no evidence that the presence of crabs varies with distance along the shoreline (p = 0.364). 

For each additional unit of distance, the odds of crab presence increase by a factor of exp(0.05804) â‰ˆ 1.06, though this effect is not statistically significant (p = 0.364).

## Back transformation and visualising our model estimates

When we get a log-odds (logit) coefficient from logistic regression, itâ€™s not directly interpretable as a probability. We need to backtransform it using the logistic function:

Step-by-step conversion:

1. Log-odds to odds:

Take the exponent of the log-odds value to get to odds

<!-- TODO eqation -->
 
Odds are the ratio of success to failure (e.g., if odds = 2, success is twice as likely as failure).

2. Odds to probability:

<!-- TODO eqation -->

This ensures the probability is always between 0 and 1.

Luckily, `estimate_means()` does this for us and we don't have to do the math to backtransform to probability for a binomial distribution
```{r}
estimate_means(fit_crab, by = "Dist")
```

# GLM Count

Let's work with some data collected by some students our department who had surveyed a beach. This group of students wanted to analyse **how the presence of a crab varied with time and distance from the water line**. 

The response variable is **discrete**: 

- `Soleolifera` is the latin name for the order of terrestrial slugs. This column gives us the counts of this taxa.

The predictor is **discrete**:

- `Treatment` 

## Worked Example: Slugs during revegetation

## Read in data

Let's load the data and have a look at the first few rows of the dataset

```{r}
data_revegetation <- read_csv("data/Revegetation.csv")

data_revegetation
```

## Look at raw data

```{r}
ggplot(data = data_revegetation, aes(x = Treatment, y = Soleolifera))  + 
  geom_violin(fill = NA) + 
  geom_jitter(aes(colour = Treatment),alpha = 0.5, width = 0.2, size = 2 ) +   theme_minimal() + 
  coord_flip()
```

## Fit the model

```{r}
slug_fit <- glm(Soleolifera ~ Treatment, family = "poisson", data = data_revegetation)
```

## Look at model output

```{r}
parameters(slug_fit)
```

Tests indicate strong evidence of a treatment effect (p<0.001). To extract the model equation we can look at the coefficients from the fit.

## Back transformation and visualising our model estimates

The default link function for Poisson is log(). If we write the mean count as $Î»$

$log(Î»)=âˆ’0.92+2.12Ã—Treatment$

```{r}
estimate_means(slug_fit)
```

There is strong evidence of positive effect of bush regeneration on the abundance of slugs from the order Soleolifera (p < 0.001)

# Over to you!
## Over to you! GLM - Binomial data
## Example 1: Mites 

**Exercises:**

Fit a logistic regression model of the presence/absence of Galumna sp. `pa` as a function of substrate density `SubsDens` as follows, using the `glm()` function and the `family` argument:


```{r}
mites <- read_csv("data/mites.csv")

ggplot(mites, aes(x = SubsDens, y = pa)) + 
  geom_point() + 
  theme_minimal()

mite_subs_fit <- glm(pa ~ SubsDens,
                 data = mites,
                 family = "binomial")

parameters(mite_subs_fit)
estimate_means(mite_subs_fit, by = "SubsDens")
```

**Questions:**

1. What type of variable is `pa`, why do we need a GLM to model this data?

`pa` is a binary variable, 1 indicates presence/absence., 0 indicates absence/presence. We need a GLM to model this data, because this data is not continous/random and comes from a non-normal/normal distribution and cannot be represented by a linear model. 

2. What does the Intercept represent in this model?

The Intercept in this context represents the probability of observing the mite when substrate density is 1/0/100

3. What sign is the slope? What does this mean for `pa`? 

The slope SubsDen is positive/negative this means the probability of observing the mite increases/decreases with substrate density

4. Interpret the Pp-value for the slope `SubDens`

The p-value of 0.360/0.123/0.915 for the slope `SubDens` indicates that is moderate/low/high probability there is no/strong relationship between the presence and absence of mites and substrate density

## Example 2: Bacterial treatment 

Using the bacteria dataset (from the MASS package), model the presence of H. influenzae as a function of week of test. Start with a full model and reduce it to the most parsimonious model.

This dataset was made to test the presence of the bacteria H. influenzae in children with otitis media in the Northern Territory of Australia. Dr A. Leach tested the effects of a drug on 50 children with a history of otitis media in the Northern Territory of Australia. The children were randomized to the drug or a placebo. The presence of H. influenzae was checked at weeks 0, 2, 4, 6 and 11: 30 of the checks were missing and are not included in this data frame.

Load the MASS package and the bacteria dataset:

```{r}
# install.packages("MASS")
library(MASS)

bacteria

bacteria_fit <- glm(y ~ week, data = bacteria, family = binomial)

parameters(bacteria_fit)
estimate_means(bacteria_fit, by = "week")
```

**Exercises:**

**Questions:**

1. What type of variable is `y`? What distribution can we use to represent it well?

`y` is the presence/absence/ count/density of the bacteria H. influenzae. A binomial/Poisson/normal distribution will represent this type of data well. 

2. Interpret the slope `week`

The negative/positive value for the slope indicates that the probability of detecting the bacteria H. influenzae decreases as weeks pass.

3. What is the p-value for the slope `week` represent?

The p-value for the slope `week` represents the probabilty there is no/strong/weak relationship between the presence of bacteria H. influenzae/the intercept with week/age/location.

4. What does the p-value for the slope `week` mean then?

The p-value is  0.011/0.1/11 this indicates there is relatively low probabilty that there is no relationship between the presence of bacteria H. influenzae and week.

## Over to you! GLM - Count data
## Example 1: Faramea trees in Panama

In this dataset, the number of trees of the species Faramea occidentalis was measured in 43 quadrants in Barro Colorado Island in Panama. For each quadrant, environmental characteristics were also recorded such as elevation or precipitation. Let us take a look at the number of Faramea occidentalis found at each quadrant.

**Exercises:**

```{r}
faramea <- read_csv("data/faramea.csv")

ggplot(faramea, aes(x = Faramea.occidentalis)) +
  geom_histogram() + 
  theme_minimal()

# Fit a Poisson GLM
faramea_fit  = glm(Faramea.occidentalis ~ Elevation,
  data = faramea, family = poisson) 

parameters(faramea_fit)
```


**Questions:**
1. Make a histogram of `Faramea.occidentalis` in the data `faramea`. What type of data is this? 

2.
3.
4.
## Example 2: Mite counts with water content

Use the mites dataset! Model the abundance of the species Galumna as a function of the substrate characteristics (water content WatrCont and density SubsDens)


**Exercises:**

```{r}
mites <- read_csv("data/mites.csv")

glm.p = glm(Galumna~WatrCont, data=mites, family=poisson)
```


**Questions:**
1.
2.
3.
4.

