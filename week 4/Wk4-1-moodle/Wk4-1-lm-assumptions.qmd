---
title: "Week 4-1 Linear model assumptions"
output: html_document
#   moodlequiz::moodlequiz:
#     replicates: 1
# moodlequiz:
#   category: "Week 4-1 Linear model assumptions"
# editor_options: 
#   chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# remotes::install_github("numbats/moodlequiz")
library(moodlequiz)

# Libraries for prac
library(tidyverse)
library(easystats)

# Data files for prac
# data_hooker <- read_csv("week 4/Wk4-1-moodle/data/Hooker.csv")
# data_growthrate <- read_csv("week 4/Wk4-1-moodle/data/GrowthRate.csv")
#data_brains <- read_csv("week 4/Wk4-1-moodle/data/Brain.csv")
# data_amphipods <- read_csv("week 4/Wk4-1-moodle/data/Amphipod_fecundity.csv")
# data_riverflow <- read_csv("week 4/Wk4-1-moodle/data/riverflow.csv")
# data_koalas <- read_csv("week 4/Wk4-1-moodle/data/koalas.csv")
```

# Introduction to linear models

## Introduction

In the previous practical, we learned about using linear models to understand how `y` varies with a continuous `x` variable and a discrete `x` variable ...but how do we know our models are any good? Can we evaluate the validity of our linears models? 

In this practical, we will build on what we learned last week on fitting linear modeles. We will build on this knowledge and get you use to making interpretations. We will explore approaches in assessing our model fits and learn ways on how to improve our model fits using transformation. Throughout we will work with various real-world datasets. By the end of today, youâ€™ll have a solid foundation for applying these techniques to models you will be fitting in your own work.

## Key learning objectives

Our learning objective today are:

- **run** a **linear regression** in R using `lm()`
- **interpret** the output of a **linear regression** 
- **run diagnostic checks** on your  model output using `check_model()` to assess whether it meets the assumptions of a linear model.

<!-- using: -->
<!--   - `summary()` -->
<!--   - `parameters()` -->
<!--     - **extract** the **confidence interval** for the **slope** and **intercept** of the **regression** line -->
<!--   - `estimate_means()` -->
<!-- - **plot** the data and the **regression** line -->
<!--   - add the **confidence interval** using output of `estimate_means()` -->

Letâ€™s dive in! ðŸš€ 

![Image credit:  @allison_horst](images/r-learners.png){width=80%} <br>

## Setting up

**Materials:**

Everything you need for this prac is on Moodle

1. Download this week's materials zip file from Moodle, from the course page
2. Unzip the file by: 
  - MacOS: Double clicking the zipfile 
  - Windows: Right click on the zip file and click "Extract All" 
3. Move the extracted folder into the folder where you store materials for `BEES2041/` 
4. **Click on the Rstudio project file, eg. `Wk-4-1-lm-assumptions.Rproj`** to open the RStudio project and you're in!!!

We will be working with various datasets collected. These are in the folder `data/`.

You will work in the relevant Quarto document for this prac. Within each Quarto docs there are several challenges for you to complete on your own devices in order to **answer the questions on Moodle**.

**Setting up: Packages:**

We will also be working with packages from the `tidyverse` and `easystats`, building on skills from previous pracs. You should have these already installed on your machines.

> **Note** that when running R in the browser we need to install the packages each time we start a new session or after you've checked your answer

In case you don't have them installed, here is code for you to do so:
```{r, eval=FALSE}
# Uncomment and run only the lines below only if you have not previously installed these.
# install.packages("tidyverse")
# install.packages("easystats")
```

> Remember to load the packages into R to use em! 

```{r, results='hide', warning=FALSE, message=FALSE}
library(tidyverse)
library(easystats)
```

# LM continuous (Leaves)

## Worked example - Isaac with leaves across climates

![Remember Dr. Isaac Towers?! Image credit: Data Dan](images/isaac-plant-ecosystems.png){width=60%} <br>
We will be working with an example from Dr. Isaac Towers - a postdoc in the School of Biological, Earth & Environmental Sciences. Specifically, Isaac wants to know: How does a leaf's mass per area change with rainfall? His research is published in New Phytologist, you can have a read of his original study [here](https://nph.onlinelibrary.wiley.com/doi/10.1111/nph.19478). We will be working with the data Isaac compiled for us.  Recall, last week we ran a linear model using `log10_leaf_mass_per_area`, `log10_rainfall`, let's try run the same linear model again but with the raw, untransformed data.

## Read in data from a URL link

```{r}
# The link the contains our data
url <- "https://raw.githubusercontent.com/nicercode/BEES2041/refs/heads/main/week%203/Wk3-1-Linear-models/data/towers-2024.csv" 

# Read in the data, drop columns where their names start with `log10`
data_towers <- read_csv(url) |> 
  select(-starts_with("log10"))
```

## Fit model

```{r}
towers_og_fit <- lm(leaf_mass_per_area ~ rainfall, data = data_towers)
```

## Check model assumptions

Recall from lectures this week that the assumptions of a linear model assumes:

1. **Linear** relationship between `x` and `y` 
2. **Residuals** are **normally** distributed
3. **Constant variance** of residuals (homogeneity of variance)
4. No major **outliers** / influential observations
5. **Independence** of points (sampling design)
6. Our **samples represent the population** of interest? (sampling design)

The last two assumptions has two do with the sampling design. You can only check the validity of these by carefully considering the how data was collection. The remaining assumptions we can check using the the outputs of `check_model()`.

> Note, you may need to click on "Zoom" to give the plots some room to breathe

```{r}
check_model(towers_og_fit)
```

In [this week's lectures](https://youtu.be/zzFyjTnAa8Y?si=LpkRecpF0LSVu91N), Data Data goes through how to use the four main plots neccessary for checking the assumptions of a liner model: 

1. **Linearity**
    - Check the relationship between fitted values and residuals
        - Should see no structure or patterns/curvature

2. **Normality of Residual**
    - Check the Quantile-Quantile plot
        - Sample points should fall along the line

3. **Homogeneity of Variance**
    - Check the relationship between fitted values and square root of standardised residuals
        - Line should be flat and horizontal
        - No clear pattern in points e.g. fanning out or grouping

4. **Influential Observations**
    - Check the leverage of each point
      - Points should fall within dashed lines

**How to report on your model checks**

Generally speaking, model checks are pretty standard and not very exciting. Typically

> Usually we don't include these in a publication or final report. Could include in eSupp. We just need to say and explain what checks were done and how you amended your data if neccessary to improve fit

## Make some adjustments to improve model fit

There are two ways we can improve the model fit for a linear model: 

- apply a transformation on the raw data
  - log, typically, log 10 `log10(var)` 
  - square root `sqrt(var)`
- if any, remove outliers 

Here we will focus on applying a `log10` transformation to two relevant variables `leaf_mass_per_area` and `rainfall`. We will be creating a new variable using `mutate()` and will be assigning these new changes to `data_towers` into a new data object called `data_towers_transformed`

```{r}
data_towers_transformed <- data_towers |>  # Assign changes in data_towers into a new dataset
  mutate(log10_leaf_mass_per_area =  log10(leaf_mass_per_area), # Create a new log10 transformed leaf mass per area
         log10_rainfall = log10(rainfall)) # Create a new log10 transformed rainfall
```

## Fit model again

Let's try fit out model again using our transformed variables
```{r}
towers_transformed_fit <- lm(log10_leaf_mass_per_area ~ log10_rainfall, data = data_towers_transformed)
```

## Check model assumptions again

Use `check_model()` on our new model fit using transformed data. How does it look?! 

```{r}
check_model(towers_transformed_fit)
```

# Over to you

For the remainder of the practical, we will get you to: 

- fit various linear models
- check their assumptions
- make changes to the data to improve the model fit and adherance to assumptions
- make some interpretations on the results of the model

We have not provided the step-by-step instructions / R code for you to complete these exercises. Use the knowledge you've gained so far and apply them here. Make use of your saved code from the previous pracs and the worked example above if you need some guidance. 

> Check out the dplyr data transformation/manipulation cheatsheet [here](https://rstudio.github.io/cheatsheets/data-transformation.pdf)

![](images/data-transformation-cheatsheet-thumbs.png){60%}

# Example 1: Growth rates vs. individual size

The file `data/GrowthRate.csv` contains 138 observations on the relationship between individual growth rate `GrowthRate` of a wide range of organisms and average individual weight `BodyWt` . The dataset contains species whose size varies over 14 orders of magnitude from approximately 10-9 to 106 g. 

> We want to know whether body weight affects growth rate in these species. 

**Exercise:**

Use R to:
- plot a graph of `GrowthRate` versus `BodyWt` with `ggplot2`
- make adjustments to your data, you may want to plot again to check whether your adjustments helped 
- conduct the linear regression analysis using `lm()` 
- evaluate whether your model fit is in line with the assumptions of a linear model using `check_model()`

```{r}
library(tidyverse)
library(easystats)

# Read in data
data_growthrate <- read_csv("data/GrowthRate.csv")

# Plot raw data
ggplot(data_growthrate, aes(x = BodyWt, y = GrowthRate)) +
  geom_point() # Data is widespread and should be log10 transformed

# Fit a linear model
growth_bodywt_fit <- lm(GrowthRate ~ BodyWt, data = data_growthrate)

# Check model
check_model(growth_bodywt_fit)

# Transform data
data_growthrate_transformed <- data_growthrate |> 
  mutate(log10_bodywt = log10(BodyWt),
         log10_growthrate = log10(GrowthRate))

# Plot transformed data
ggplot(data_growthrate_transformed, aes(x = log10_bodywt, y = log10_growthrate)) +
  geom_point() 

# Fit a linear model
growth_bodywt_transformed_fit <- lm(log10_growthrate ~ log10_bodywt, data = data_growthrate_transformed)

# Check model
check_model(growth_bodywt_transformed_fit)
```

**Questions**
1. What is the equation for the linear model you are considering here?

`r cloze("GrowthRate = b0 + b1 * BodyWt", c("BodyWt = b0 + b1 * GrowthRate", "BodyWt = b0 + b1 + GrowthRate","GrowthRate = b0 + b1 * BodyWt", "GrowthRate = b0 + b1 + BodyWt"))`

2. Based on your plot of the raw data, what are some potential problems you see in fitting this linear model to these data? Select all that applies: 

```{r, include=FALSE}
possible_assumption_issues <- c("high variance in larger values", 
                                "there may be some outliers, but hard to tell because the data has a massive spread",
                                "the variables are non-normal",
                                "x and y may not follow a linear relationship", 
                                "All looks good to me")
```

The potential issues with fitting a linear model to this data include: 
`r cloze(c("high variance in larger values", "x and y may not follow a linear relationship", "there may be some outliers, but hard to tell because the data has a massive spread"), possible_assumption_issues)`

3. What transformation do you recommend for this dataset?

```{r, include=FALSE}
possible_transformations <- c("log 10", "natural log", "square root", "log 10 (var + 0.1)")
```

A `r cloze("log 10", possible_transformations)` transformation is suited for this dataset because observation values vary by orders of magnitude. 

4. How did the transformation change the "Linearity" `check_model()` plot?

The log 10 transformation `r cloze("put the fitted and residual values on a similar scale so we can see them spread out", c("removed the outliers", "put the fitted and residual values on a similar scale so we can see them spread out", "made the results non-significant"))`

# Example 2: Brain weight vs. body size

![](images/brains.png)

The data file `data/Brain.csv` contains data on average brain weight for 62 species of mammals. The file presents body weight (kg) `BodyWt` and brain weight (g) `BrainWt` for each species.

> We wish to consider the problem of modelling brain weight (y response) as a function of body weight (x predictor). 

**Exercises:**

- Use R to draw a scatter plot of Brain weight versus Body weight. 
- Note down on any problem you see in fitting a linear model to these data.

```{r}
library(tidyverse)
library(easystats)

# Read in data
data_brains <- read_csv("data/Brain.csv")

# Plot raw data
ggplot(data_brains, aes(x = BodyWt, y = BrainWt)) +
  geom_point() # Data is widespread and should be log10 transformed

# Fit a linear model
brains_bodywt_fit <- lm(BrainWt ~ BodyWt, data = data_brains)

# Check model
check_model(brains_bodywt_fit)

# Apply transformations
data_brains_transformed <- data_brains |> 
  mutate(log10_bodywt = log10(BodyWt),
         log10_brainwt = log10(BrainWt))

# Plot transformed data
ggplot(data_brains_transformed, aes(x = log10_bodywt, y = log10_brainwt)) +
  geom_point() 

# Refit model 
brains_bodywt_transformed_fit <- lm(log10_brainwt ~ log10_bodywt, data = data_brains_transformed)

# Check model
check_model(brains_bodywt_transformed_fit)

# Inspect model summmaries
```

**Questions**

1. Find transformations of one or both variables so that on the transformed scale the regression is linear. Fit an appropriate linear regression model to the transformed data.

What transformations help this data make a linear relationship?

2. Now, create a linear model with your transformed data. 

Examine the graph of residuals from your model versus fitted values. Comment on your residual plot comparing the linear model with untransformed data to the linear model with transformed data.

3. Humans are considered to be a big brained species. Is there evidence from the analysis to suggest that humans are an unusual species relative to the others? Could we consider it as an 'outlier'?

> Hint: Use residuals plot to check for outliers

# Example 3: Boiling point of water vs. air pressure

![Ever notice that water boils quicker at high altitudes? Image credit: Slower Hiking](images/boiling-water-at-elevation.jpg){60%}

In the 1840's, the Scottish physicist Dr James Forbes travelled to the Swiss Alps and made many measurements of how the boiling point of water varied with air pressure (which decreases with altitude). In his research paper , Forbes also presented data collected on the measures of the same two quantities by Dr Joseph Hooker (famous botanist, explorer, friend of Charles Darwin). Unlike Forbes, however, Hooker took his measurements in the Himalayan Mountains, generally at higher altitudes.

The data we will explore here are a subset of Hooker's data containg 31 observations:
- `Temperature` at boiling point of water (degrees Fahrenheit)
- `Pressure`, the corrected barometric pressure (inches of mercury). 

> We want to know whether changes in pressure (which decreases with altitude) affects the boiling point of water. 

Start by reading the data  `data/Hooker.csv` into R.

**Exercise:**

Use R to:
- plot a graph of Temperature versus Pressure with `ggplot2`
  - consider labelling your points with the row number of each value
- conduct the linear regression analysis using `lm()` (Temperature = a + b*Pressure)
- evaluate whether your model fit is in line with the assumptions of a linear model using `check_model()`

```{r}
library(tidyverse)
library(easystats)

# Read in data
data_hooker <- read_csv("data/Hooker.csv")

# Plot raw data
data_hooker |> 
  mutate(obs_id = row_number()) |>  # Create a temporary variable that denotes the row number of each observation
ggplot(aes(x = Pressure, y = Temperature)) + 
  geom_text(aes(label = obs_id)) # A scatterplot where each point is the observation number
  theme_classic()

# Fit the model with all data
hooker_fit <- lm(Temperature ~ Pressure, data = data_hooker)

# Check model
check_model(hooker_fit)

# Exclude potential outlier
data_hooker_norow7 <- data_hooker |> 
   slice(-7)

# Fit the model with all data
hooker_norow7_fit <- lm(Temperature ~ Pressure, data = data_hooker_norow7)

# Check model
check_model(hooker_norow7_fit) # Not really improve things..

# Transform data instead
data_hooker_transformed <- data_hooker |> 
  mutate(log10_temp = log10(Temperature),
         log10_pressure = log10(Pressure))

# Fit the model with all data
hooker_transformed_fit <- lm(log10_temp ~ log10_pressure, data = data_hooker_transformed)

# Check model
check_model(hooker_transformed_fit) # There is an improvement but probably good to remove outlier

# Remove outlier in transformed data
data_hooker_transformed_norow7  <- data_hooker_transformed |> 
   slice(-7)

# Fit the model with all data
hooker_transformed_norow7_fit <- lm(log10_temp ~ log10_pressure, data = data_hooker_transformed_norow7)

# Check model
check_model(hooker_transformed_norow7_fit) # Heaps better!
```

**Questions**
1. Is there evidence of violating assumptions, through outliers, non-linearity or non-constant error variance?

2. You notice that point 7 looks a bit out of place an.After some investigation, it turns out there were machinery issues when the data was recorded. You decide that's sufficient reason to exclude the point as an outlier. 

In `dplyr`, the function `slice` gives us a subset of rows by row number. Placing a minus `-` sign in front of that number will give us all rows **except** that one listed. To remove row 7, we would use:

```{r}
data_hooker_norow7 <-  data_hooker |> slice(-7)
```

Did removing the outliers improve any problems with this dataset?

Rerun your linear model with your new dataset and check your model again

3. Removing the outlier didn't really help... let's try transforming the data. `log10` transform your x and y variable and rerun the linear model analysis and model checks

Recall to use `mutate()` to create a new variable containing your transformed data.

- Write down the fitted equation: log10(Temp) = XX + XX * log10(Pressure)
- What is the F-value
- What is the P-value

4.  Look at the the residual plots from `check_model(my_model)` and compare it between: 

- a model with row 7
- a model without row 7
- a model with log transformed data with row 7

Can you see any evidence of outliers in the log-transformed data? What data adjustments would you recommend for a linear model for the Hooker dataset? 

# Example 4: Body size vs fecundity

![](images/amphipods.jpeg){60%}

An evolutionary biologist was interested in body size-fecundity relationships in marine invertebrates. For most animals, larger females are able to produce more offspring per reproductive event. This is due to the levels of resources available for allocation to reproduction, and for species that brood their offspring, the physical constraints of housing the developing eggs or embryos.

Many crustaceans have indeterminate growth, which means they just keep on moulting and getting bigger until something kills them. Consequently, older and larger females can have many more offspring per reproductive event than smaller individuals. Amphipods are crustaceans that do not release their eggs into the plankton, but look after them in a brood pouch between their legs.

To explore how fecundity varied with body size, the biologist collected 40 females that were brooding eggs, measured their body length `Length` (mm) and the number of eggs `Eggs` in their brood pouch.

Load the data file `data/Amphipod.fecundity.csv` and choose a plot to visualise egg number per brood, you may want to label each point as the row number again.

```{r}
library(tidyverse)
library(easystats)

# Read in data
data_amphipods <- read_csv("data/Amphipod_fecundity.csv")

# Plot raw data
data_amphipods |>
  mutate(obs_id = row_number()) |>  # Create a temporary variable that denotes the row number of each observation
ggplot(aes(x = Length, y = Eggs)) + 
  geom_text(aes(label = obs_id)) +
  theme_classic()

# Fit the model with all data
amphipods_fit <- lm(Eggs ~ Length, data = data_amphipods)

# Check model
check_model(amphipods_fit)

# Exclude potential outlier
data_amphipods_norow40 <- data_amphipods |> 
   slice(-40)

# Fit the model with all data
amphipods_norow40_fit <- lm(Eggs ~ Length, data = data_amphipods_norow40) 

# Check model
check_model(amphipods_norow40_fit) # Looks pretty good!

# Interpret model output
parameters(amphipods_norow40_fit)
summary(amphipods_norow40_fit)
```

**Questions**

1. What analysis would you use to describe the relationship between body length and egg number per brood?

Run the suitable analysis. Check your model assumptions and make any data adjustments you see fit then answer the following:

2.  What is the probability that there is no relationship between body size and body length?

3. How many more eggs are produced per brood for every 1 mm increase in body length?

4. How many eggs would you predict that a 10 mm female would have?

5. What percentage of the variance in egg number per brood is explained by your relationship?


# Example 5: River flow and density of larval fish

![](images/retropinna-semoni-cropped.jpg) <br>

River managers want to know if there is an difference in the numbers of fish recruiting under different environmental flow practices. They go out to a number of different locations along a river of interest and measure the density of larval fishes and water flow rate (velocity m/s) at each site.

Load the data file `data/riverflow.csv` and choose a plot to visualise the relationship between flow rate `Flow.rate` and density of fish larvae `Larvae.density`.

```{r}
library(tidyverse)
library(easystats)

# Read in data
data_riverflow <- read_csv("data/riverflow.csv")

# Plot raw data
ggplot(data_riverflow, aes(x = Flow.rate, y = Larvae.density)) +
  geom_point() 

# Fit a linear model
riverflow_fit <- lm(Flow.rate ~ Larvae.density, data = data_riverflow)

# Check model
check_model(riverflow_fit) # Not bad!

# Inspect model summmaries
parameters(riverflow_fit)
summary(riverflow_fit)
```

**Questions**

1. What analysis would you use to describe the relationship between flow velocity and fish larvae density?

Run the suitable analysis. Check your model assumptions and make any data adjustments you see fit then answer the following:

2. What is the probability that there is no relationship between fish larvae density and water flow velocity?

3. How many more fish larvae are present for every unit increase in flow velocity?

4. What is the sign of the slope?

5. What does this mean?

6. What percentage of the variance in larvae fish density is explained by your relationship?

# Example 6: Koalas and fire regimes

![](images/koala-fires.jpeg){60%}

A group of forest managers want to know if there is any difference in the numbers of koalas under control fire practices. To do this, they surveyed the number of koalas from mixed Eucalyptus forest with varying species composition due to fire history. They then found records of the most recent fire at each survey site to use as a measure of fire disturbance.

Load the data file `data/koalas.csv` and choose a plot to visualise the number of koalas observed `Koalas` and years since fire `Year.last.fire`.

```{r}
library(tidyverse)
library(easystats)

# Read in data
data_koalas <- read_csv("data/koalas.csv")

# Plot raw data
ggplot(data_koalas, aes(x = Year.last.fire, y = Koalas)) +
  geom_point() 

# Fit a linear model
koalas_fit <- lm(Koalas ~ Year.last.fire, data = data_koalas)

# Check model
check_model(koalas_fit) # Not bad!

# Inspect model summmaries
parameters(koalas_fit)
summary(koalas_fit)
```


**Questions**

1. What analysis would you use to describe the relationship between the number of koalas observed and the year of the most recent fire?

Run the suitable analysis. Check your model assumptions and make any data adjustments you see fit then answer the following:

2. What is the slope of our model?

3. What is the sign of the slope?

4. What does this mean?

5. What is the probability that there is no relationship between the year of the most recent fire and the number of koalas?

6. What percentage of the variance in koala numbers is explained by the year of the most recent fire?
